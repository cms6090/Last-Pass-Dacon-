{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a8d2ecb",
   "metadata": {},
   "source": [
    "### ============================================================\n",
    "### AutoGluon Tabular 기반 end_x/end_y 회귀 파이프라인 (수정본)\n",
    "1) GroupKFold(game_id) OOF 평가(유클리드)\n",
    "2) object 결측치 처리 일관화 (학습/OOF/importance/최종/테스트)\n",
    "3) 임시 predictor 폴더 try/finally 정리\n",
    "4) (누수 완화) feature importance pruning을 CV-train에서만 누적\n",
    "5) branching OOF: valid subset만 예측 + 안전장치\n",
    "6) OOF 결과 디스크 캐시 (후보 비교 반복 학습 낭비 제거)\n",
    "7) k_prev 탐색은 cheap preset(빠르게), 본 탐색은 good, 최종은 best\n",
    "### ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b2b706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy pandas tqdm catboost scikit-learn\n",
    "# !pip install -U autogluon\n",
    "# !pip install -U torch torchvision --index-url https://download.pytorch.org/whl/cu130\n",
    "\n",
    "import os, json, shutil, hashlib, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from autogluon.tabular import TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6904440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_GPUS = 1\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# GPU 확인 (환경 점검용)\n",
    "# -------------------------\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "def get_num_gpus():\n",
    "    \"\"\"\n",
    "    GPU 자동 감지:\n",
    "      - torch import 가능 + cuda available -> 1\n",
    "      - 아니면 0\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        return 1 if torch.cuda.is_available() else 0\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "NUM_GPUS = get_num_gpus()\n",
    "print(\"NUM_GPUS =\", NUM_GPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45a51e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 경로 설정\n",
    "# -------------------------\n",
    "ART_DIR       = \"artifacts\"    # preprocess에서 만든 parquet 폴더\n",
    "MODEL_DIR     = \"models_ag\"    # 최종 predictor 저장 폴더\n",
    "TMP_DIR       = \"ag_tmp\"       # fold별 임시 predictor 저장 폴더\n",
    "OOF_CACHE_DIR = \"oof_cache\"    # OOF 캐시 폴더\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(TMP_DIR, exist_ok=True)\n",
    "os.makedirs(OOF_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# 경기장 상수(좌표 클리핑/후처리에 사용)\n",
    "# -------------------------\n",
    "PITCH_X, PITCH_Y = 105.0, 68.0\n",
    "GOAL_Y = 34.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac3f481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Global option: parallel/sequential\n",
    "#   - \"parallel_local\": AG 내부 bagging/stacking fold 전략을 parallel로\n",
    "#   - \"sequential_local\": 안정성 우선\n",
    "#   - None: AG 기본값\n",
    "# -------------------------\n",
    "FOLD_FITTING_STRATEGY = \"sequential_local\"  # 필요시 \"sequential_local\"로 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0af46b2",
   "metadata": {},
   "source": [
    "### Utils: 평가/클리핑/데이터 로딩/전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46ee79f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_mean_distance(y_true_xy: np.ndarray, y_pred_xy: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    (end_x, end_y) 2차원 예측의 평균 유클리드 거리.\n",
    "    - 최종 평가 지표(OOF 비교/모델 선택/후처리 튜닝)에 사용\n",
    "    \"\"\"\n",
    "    diff = y_true_xy - y_pred_xy\n",
    "    return float(np.sqrt(diff[:, 0]**2 + diff[:, 1]**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "215c0664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_xy(px, py):\n",
    "    \"\"\"\n",
    "    예측 좌표를 경기장 범위로 강제 클리핑\n",
    "    - x: [0, 105], y: [0, 68]\n",
    "    - OOF/테스트 예측 모두에 적용해 비현실적인 값 방지\n",
    "    \"\"\"\n",
    "    return np.clip(px, 0, PITCH_X), np.clip(py, 0, PITCH_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1035f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_object_missing(df: pd.DataFrame, cols=None, fill_value=\"MISSING\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    object 컬럼의 NaN을 일관되게 채우기\n",
    "\n",
    "    - 원 코드에선 X_all만 채우고 실제 학습 df(tr/va/train_full)는 안 채워져서 불안정 가능\n",
    "    - OOF/importance/최종학습/테스트 모두 여기로 통일\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    use_cols = cols if cols is not None else out.columns\n",
    "    for c in use_cols:\n",
    "        if c in out.columns and out[c].dtype == \"object\":\n",
    "            out[c] = out[c].fillna(fill_value)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0653ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pack(k_prev: int):\n",
    "    \"\"\"\n",
    "    preprocess에서 저장한 k_prev 버전의 train/test feature + label을 로드\n",
    "    - 입력: artifacts/features_train_k{k}.parquet, labels_train_k{k}.parquet, features_test_k{k}.parquet\n",
    "    - 출력: X_train(피처), y_train(라벨), X_test(피처)\n",
    "    \"\"\"\n",
    "    X_train = pd.read_parquet(os.path.join(ART_DIR, f\"features_train_k{k_prev}.parquet\"))\n",
    "    y_train = pd.read_parquet(os.path.join(ART_DIR, f\"labels_train_k{k_prev}.parquet\"))\n",
    "    X_test  = pd.read_parquet(os.path.join(ART_DIR, f\"features_test_k{k_prev}.parquet\"))\n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91658136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X_train, y_train):\n",
    "    \"\"\"\n",
    "    학습용 테이블(data)을 만들고, GroupKFold를 위한 groups(game_id)와 feature 컬럼 리스트를 구성.\n",
    "    AutoGluon은 fit(train_data=...)에 label 컬럼이 포함된 df를 넣어야 하므로 merge된 data를 중심으로 사용.\n",
    "\n",
    "    - data: (피처 + end_x/end_y 라벨 포함) merged df\n",
    "    - groups: GroupKFold 분리용 game_id\n",
    "    - feat_cols: 모델 입력으로 사용할 피처 컬럼 목록\n",
    "    \"\"\"\n",
    "    # 1) episode 단위로 피처와 라벨을 합침\n",
    "    data = X_train.merge(y_train, on=\"game_episode\", how=\"inner\")\n",
    "\n",
    "    # 2) group split 단위는 game_id (같은 game의 episode들이 fold를 넘나들지 않게)\n",
    "    groups = data[\"game_id\"].values\n",
    "\n",
    "    # 3) feature 컬럼은 label/키 제외\n",
    "    drop_cols = {\"game_episode\", \"end_x\", \"end_y\"}\n",
    "    feat_cols = [c for c in data.columns if c not in drop_cols]\n",
    "\n",
    "    # 4) 참고용(평가용 라벨 배열)\n",
    "    yx = data[\"end_x\"].values\n",
    "    yy = data[\"end_y\"].values\n",
    "\n",
    "    return data, yx, yy, groups, feat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3117573f",
   "metadata": {},
   "source": [
    "### 0-1) OOF 캐시 유틸 (후보 비교 반복 학습 제거)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4764ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_oof_key(k_prev, feat_cols, x_metric, y_metric, n_splits, tag=\"base\"):\n",
    "    \"\"\"\n",
    "    OOF 캐시 키 생성\n",
    "    - feature 리스트가 길어서 md5 hash로 축약\n",
    "    - tag로 목적(예: 'cand', 'ksearch') 구분 가능\n",
    "    \"\"\"\n",
    "    h = hashlib.md5((\"|\".join(feat_cols)).encode(\"utf-8\")).hexdigest()[:10]\n",
    "    return f\"{tag}_k{k_prev}_f{h}_{x_metric}_{y_metric}_cv{n_splits}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e04f17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_oof_cache(key):\n",
    "    \"\"\"캐시 존재 시 (score, oof_pred) 반환\"\"\"\n",
    "    path = os.path.join(OOF_CACHE_DIR, f\"{key}.pkl\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6599ed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_oof_cache(key, obj):\n",
    "    \"\"\"(score, oof_pred) 저장\"\"\"\n",
    "    path = os.path.join(OOF_CACHE_DIR, f\"{key}.pkl\")\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b04f9c0",
   "metadata": {},
   "source": [
    "### 1) AutoGluon helper: fold 1개 학습 / OOF 생성 / OOF 점수 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1b3e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ag_fit_one_fold(\n",
    "    train_df: pd.DataFrame,\n",
    "    label: str,\n",
    "    eval_metric=\"rmse\",\n",
    "    presets=\"good_quality\",\n",
    "    time_limit=None,\n",
    "    path=None,\n",
    "    num_gpus=NUM_GPUS,\n",
    "    fold_fitting_strategy=None,   # \"parallel_local\" / \"sequential_local\" / None\n",
    "    extra_fit_kwargs=None,        # dict 형태로 추가 fit kwargs 주입 가능\n",
    "):\n",
    "    \"\"\"\n",
    "    (단일 fold) AutoGluon TabularPredictor 학습\n",
    "\n",
    "    - train_df: feature + label이 포함된 DF\n",
    "    - label: 학습할 타깃 컬럼명 (\"end_x\" 또는 \"end_y\")\n",
    "    - eval_metric: AutoGluon이 내부 모델 선택/평가에 사용할 metric (rmse/mae 등)\n",
    "    - presets: 학습 품질/시간 트레이드오프 템플릿\n",
    "    - time_limit: 이 fold 학습에 허용할 총 시간(초). None이면 제한 없음\n",
    "    - path: predictor 저장 경로(폴더). 이미 있으면 삭제 후 재생성\n",
    "    \"\"\"\n",
    "    if path is not None and os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "\n",
    "    fit_kwargs = dict(\n",
    "        train_data=train_df,\n",
    "        presets=presets,\n",
    "        time_limit=time_limit,\n",
    "        ag_args_fit={\"num_gpus\": int(num_gpus)},\n",
    "    )\n",
    "\n",
    "    if fold_fitting_strategy is not None:\n",
    "        fit_kwargs[\"ag_args_ensemble\"] = {\"fold_fitting_strategy\": fold_fitting_strategy}\n",
    "\n",
    "    if extra_fit_kwargs:\n",
    "        fit_kwargs.update(extra_fit_kwargs)\n",
    "\n",
    "    predictor = TabularPredictor(\n",
    "        label=label,\n",
    "        problem_type=\"regression\",\n",
    "        eval_metric=eval_metric,\n",
    "        path=path,\n",
    "    ).fit(**fit_kwargs)\n",
    "\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ac3dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ag_oof_predict(\n",
    "    df_all: pd.DataFrame,\n",
    "    label: str,\n",
    "    feat_cols: list,\n",
    "    groups: np.ndarray,\n",
    "    n_splits=5,\n",
    "    eval_metric=\"rmse\",\n",
    "    presets=\"good_quality\",\n",
    "    time_limit=None,\n",
    "    save_root=TMP_DIR,\n",
    "    num_gpus=NUM_GPUS,\n",
    "    fold_fitting_strategy=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    GroupKFold 기반 OOF 예측 생성.\n",
    "    - fold마다 predictor를 학습하고(valid fold에는 절대 학습 데이터가 들어가지 않음)\n",
    "      valid fold를 예측한 값을 oof 배열에 채움.\n",
    "\n",
    "    반환:\n",
    "    - oof: 길이 N(샘플 수)짜리 예측값 배열\n",
    "    \"\"\"\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    oof = np.zeros(len(df_all), dtype=float)\n",
    "\n",
    "    # gkf.split의 y는 사실상 형식상 필요(여기서는 df_all[label]을 넣음)\n",
    "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(df_all[feat_cols], df_all[label], groups=groups)):\n",
    "        # 학습 df: feature + label 포함\n",
    "        tr = df_all.iloc[tr_idx][feat_cols + [label]].copy()\n",
    "        va = df_all.iloc[va_idx][feat_cols].copy()\n",
    "        \n",
    "        # 검증 df: feature만 (예측용), object 결측 일관 처리\n",
    "        tr = fill_object_missing(tr, cols=feat_cols)\n",
    "        va = fill_object_missing(va, cols=feat_cols)\n",
    "\n",
    "        # fold별 predictor 저장 폴더\n",
    "        path = os.path.join(save_root, f\"{label}_fold{fold}\")\n",
    "        \n",
    "        try:\n",
    "            predictor = ag_fit_one_fold(\n",
    "                train_df=tr,\n",
    "                label=label,\n",
    "                eval_metric=eval_metric,\n",
    "                presets=presets,\n",
    "                time_limit=time_limit,\n",
    "                path=path,\n",
    "                num_gpus=num_gpus,\n",
    "                fold_fitting_strategy=fold_fitting_strategy,\n",
    "            )\n",
    "            \n",
    "            oof[va_idx] = predictor.predict(va).values\n",
    "        finally:\n",
    "            # ✅ 학습 중 에러가 나도 폴더는 정리\n",
    "            if os.path.exists(path):\n",
    "                shutil.rmtree(path)\n",
    "\n",
    "    return oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc0a5825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ag_cv_score_xy(\n",
    "    data_merged: pd.DataFrame,\n",
    "    feat_cols: list,\n",
    "    groups: np.ndarray,\n",
    "    x_metric=\"rmse\",\n",
    "    y_metric=\"rmse\",\n",
    "    n_splits=5,\n",
    "    presets=\"good_quality\",\n",
    "    time_limit=None,\n",
    "    num_gpus=NUM_GPUS,\n",
    "    fold_fitting_strategy=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    end_x/end_y 각각 OOF를 만든 뒤, (x,y) 유클리드 평균 거리로 점수 계산.\n",
    "\n",
    "    반환:\n",
    "    - score: mean euclidean\n",
    "    - oof_xy: shape (N,2) -> [:,0]=px_oof, [:,1]=py_oof\n",
    "    \"\"\"\n",
    "    # end_x OOF\n",
    "    px = ag_oof_predict(\n",
    "        df_all=data_merged, label=\"end_x\", feat_cols=feat_cols, groups=groups,\n",
    "        n_splits=n_splits, eval_metric=x_metric, presets=presets,\n",
    "        time_limit=time_limit, save_root=TMP_DIR, num_gpus=num_gpus,\n",
    "        fold_fitting_strategy=fold_fitting_strategy,\n",
    "    )\n",
    "\n",
    "    # end_y OOF\n",
    "    py = ag_oof_predict(\n",
    "        df_all=data_merged, label=\"end_y\", feat_cols=feat_cols, groups=groups,\n",
    "        n_splits=n_splits, eval_metric=y_metric, presets=presets,\n",
    "        time_limit=time_limit, save_root=TMP_DIR, num_gpus=num_gpus,\n",
    "        fold_fitting_strategy=fold_fitting_strategy,\n",
    "    )\n",
    "\n",
    "    # 경기장 밖으로 나간 예측은 클리핑 후 평가\n",
    "    px, py = clip_xy(px, py)\n",
    "\n",
    "    # 정답\n",
    "    y_true = data_merged[[\"end_x\", \"end_y\"]].values\n",
    "\n",
    "    # 최종 점수(평가 기준은 항상 Euclidean)\n",
    "    score = euclidean_mean_distance(y_true, np.column_stack([px, py]))\n",
    "\n",
    "    return score, np.column_stack([px, py])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ac956d",
   "metadata": {},
   "source": [
    "### 2) pruning: feature importance top-N : 누수 완화 pruning: CV-train에서만 importance 누적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bba1dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ag_prune_features_cv(\n",
    "    data_merged: pd.DataFrame,\n",
    "    feat_cols: list,\n",
    "    groups: np.ndarray,\n",
    "    n_splits=5,\n",
    "    top_n=200,\n",
    "    eval_metric=\"rmse\",\n",
    "    presets=\"good_quality\",\n",
    "    time_limit=None,\n",
    "    num_gpus=NUM_GPUS,\n",
    "    save_root=TMP_DIR,\n",
    "    fold_fitting_strategy=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    ✅ 누수 완화 pruning:\n",
    "    - 각 fold에서 \"train subset\"으로만 end_x/end_y importance 계산\n",
    "    - fold별 importance 평균을 내서 top_n 피처 선택\n",
    "    \"\"\"\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    imp_sum = pd.Series(0.0, index=feat_cols)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(data_merged[feat_cols], data_merged[\"end_x\"], groups=groups)):\n",
    "        tr_all = data_merged.iloc[tr_idx].copy()\n",
    "\n",
    "        # object 결측 처리\n",
    "        tr_all = fill_object_missing(tr_all, cols=feat_cols)\n",
    "\n",
    "        # fold별 임시 path\n",
    "        path_x = os.path.join(save_root, f\"imp_cv_endx_fold{fold}\")\n",
    "        path_y = os.path.join(save_root, f\"imp_cv_endy_fold{fold}\")\n",
    "\n",
    "        try:\n",
    "            # end_x importance\n",
    "            pred_x = ag_fit_one_fold(\n",
    "                train_df=tr_all[feat_cols + [\"end_x\"]],\n",
    "                label=\"end_x\",\n",
    "                eval_metric=eval_metric,\n",
    "                presets=presets,\n",
    "                time_limit=time_limit,\n",
    "                path=path_x,\n",
    "                num_gpus=num_gpus,\n",
    "                fold_fitting_strategy=fold_fitting_strategy,\n",
    "            )\n",
    "\n",
    "            imp_x = pred_x.feature_importance(tr_all[feat_cols], silent=True)[\"importance\"]\n",
    "\n",
    "\n",
    "            # end_y importance\n",
    "            pred_y = ag_fit_one_fold(\n",
    "                train_df=tr_all[feat_cols + [\"end_y\"]],\n",
    "                label=\"end_y\",\n",
    "                eval_metric=eval_metric,\n",
    "                presets=presets,\n",
    "                time_limit=time_limit,\n",
    "                path=path_y,\n",
    "                num_gpus=num_gpus,\n",
    "                fold_fitting_strategy=fold_fitting_strategy,\n",
    "            )\n",
    "            \n",
    "            imp_y = pred_y.feature_importance(tr_all[feat_cols], silent=True)[\"importance\"]\n",
    "\n",
    "            # fold importance 평균\n",
    "            imp_fold = (imp_x.reindex(feat_cols).fillna(0) + imp_y.reindex(feat_cols).fillna(0)) / 2.0\n",
    "            imp_sum = imp_sum.add(imp_fold, fill_value=0.0)\n",
    "\n",
    "        finally:\n",
    "            for p in [path_x, path_y]:\n",
    "                if os.path.exists(p):\n",
    "                    shutil.rmtree(p)\n",
    "\n",
    "    imp_avg = (imp_sum / n_splits).sort_values(ascending=False)\n",
    "    top_features = imp_avg.head(min(top_n, len(imp_avg))).index.tolist()\n",
    "    return top_features, imp_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199bddd9",
   "metadata": {},
   "source": [
    "### 3) branching CV: result_name으로 성공/실패 분기 학습 + 라우팅 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d29f5aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ag_cv_score_xy_branch(\n",
    "    data_merged: pd.DataFrame,\n",
    "    feat_cols: list,\n",
    "    groups: np.ndarray,\n",
    "    x_metric=\"rmse\",\n",
    "    y_metric=\"rmse\",\n",
    "    result_col=\"result_name\",\n",
    "    n_splits=5,\n",
    "    presets=\"good_quality\",\n",
    "    time_limit=None,\n",
    "    min_side=50,\n",
    "    num_gpus=NUM_GPUS,\n",
    "    fold_fitting_strategy=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    result_name 기준 분기 학습 OOF:\n",
    "    - train fold에서 Successful/Unsuccessful 별도 모델 학습\n",
    "    - valid fold는 result_name으로 라우팅\n",
    "    \n",
    "    - ✅ valid subset만 각각 예측(효율)\n",
    "    - ✅ 한쪽 표본이 너무 적으면 전체 모델로 fallback\n",
    "    \"\"\"\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    oof = np.zeros((len(data_merged), 2), dtype=float)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(data_merged[feat_cols], data_merged[\"end_x\"], groups=groups)):\n",
    "        tr_all = data_merged.iloc[tr_idx].copy()\n",
    "        va_all = data_merged.iloc[va_idx].copy()\n",
    "\n",
    "        # 전처리(학습/예측에 쓰일 feature)\n",
    "        tr_all = fill_object_missing(tr_all, cols=feat_cols)\n",
    "        va_feat = fill_object_missing(va_all[feat_cols].copy(), cols=feat_cols)\n",
    "\n",
    "        # train에서 성공/실패 분리\n",
    "        tr_s = tr_all[result_col].astype(str).eq(\"Successful\")\n",
    "        tr_u = ~tr_s\n",
    "\n",
    "        # fallback: 표본이 너무 적으면 전체 모델\n",
    "        if tr_s.sum() < min_side or tr_u.sum() < min_side:\n",
    "            # fallback: 전체 모델\n",
    "            path_x = os.path.join(TMP_DIR, f\"branch_all_endx_fold{fold}\")\n",
    "            path_y = os.path.join(TMP_DIR, f\"branch_all_endy_fold{fold}\")\n",
    "            try:\n",
    "                px_model = ag_fit_one_fold(\n",
    "                    train_df=tr_all[feat_cols + [\"end_x\"]],\n",
    "                    label=\"end_x\",\n",
    "                    eval_metric=x_metric,\n",
    "                    presets=presets,\n",
    "                    time_limit=time_limit,\n",
    "                    path=path_x,\n",
    "                    num_gpus=num_gpus,\n",
    "                    fold_fitting_strategy=fold_fitting_strategy,\n",
    "                )\n",
    "\n",
    "                py_model = ag_fit_one_fold(\n",
    "                    train_df=tr_all[feat_cols + [\"end_y\"]],\n",
    "                    label=\"end_y\",\n",
    "                    eval_metric=y_metric,\n",
    "                    presets=presets,\n",
    "                    time_limit=time_limit,\n",
    "                    path=path_y,\n",
    "                    num_gpus=num_gpus,\n",
    "                    fold_fitting_strategy=fold_fitting_strategy,\n",
    "                )\n",
    "                px = px_model.predict(va_feat).values\n",
    "                py = py_model.predict(va_feat).values\n",
    "            finally:\n",
    "                for p in [path_x, path_y]:\n",
    "                    if os.path.exists(p):\n",
    "                        shutil.rmtree(p)\n",
    "\n",
    "        # -------- 분기 학습: 성공/실패 각각 별도 predictor 학습 --------\n",
    "        else:\n",
    "            # 성공/실패 모델 각각 학습\n",
    "            path_sx = os.path.join(TMP_DIR, f\"branch_s_endx_fold{fold}\")\n",
    "            path_sy = os.path.join(TMP_DIR, f\"branch_s_endy_fold{fold}\")\n",
    "            path_ux = os.path.join(TMP_DIR, f\"branch_u_endx_fold{fold}\")\n",
    "            path_uy = os.path.join(TMP_DIR, f\"branch_u_endy_fold{fold}\")\n",
    "\n",
    "            try:\n",
    "                px_s = ag_fit_one_fold(\n",
    "                    train_df=tr_all.loc[tr_s, feat_cols + [\"end_x\"]],\n",
    "                    label=\"end_x\",\n",
    "                    eval_metric=x_metric,\n",
    "                    presets=presets,\n",
    "                    time_limit=time_limit,\n",
    "                    path=path_sx,\n",
    "                    num_gpus=num_gpus,\n",
    "                    fold_fitting_strategy=fold_fitting_strategy,\n",
    "                )\n",
    "                py_s = ag_fit_one_fold(\n",
    "                    train_df=tr_all.loc[tr_s, feat_cols + [\"end_y\"]],\n",
    "                    label=\"end_y\",\n",
    "                    eval_metric=y_metric,\n",
    "                    presets=presets,\n",
    "                    time_limit=time_limit,\n",
    "                    path=path_sy,\n",
    "                    num_gpus=num_gpus,\n",
    "                    fold_fitting_strategy=fold_fitting_strategy,\n",
    "                )\n",
    "                px_u = ag_fit_one_fold(\n",
    "                    train_df=tr_all.loc[tr_u, feat_cols + [\"end_x\"]],\n",
    "                    label=\"end_x\",\n",
    "                    eval_metric=x_metric,\n",
    "                    presets=presets,\n",
    "                    time_limit=time_limit,\n",
    "                    path=path_ux,\n",
    "                    num_gpus=num_gpus,\n",
    "                    fold_fitting_strategy=fold_fitting_strategy,\n",
    "                )\n",
    "                py_u = ag_fit_one_fold(\n",
    "                    train_df=tr_all.loc[tr_u, feat_cols + [\"end_y\"]],\n",
    "                    label=\"end_y\",\n",
    "                    eval_metric=y_metric,\n",
    "                    presets=presets,\n",
    "                    time_limit=time_limit,\n",
    "                    path=path_uy,\n",
    "                    num_gpus=num_gpus,\n",
    "                    fold_fitting_strategy=fold_fitting_strategy,\n",
    "                )\n",
    "\n",
    "                # valid 라우팅 마스크\n",
    "                va_s_mask = va_all[result_col].astype(str).eq(\"Successful\").values\n",
    "                px = np.empty(len(va_all), dtype=float)\n",
    "                py = np.empty(len(va_all), dtype=float)\n",
    "\n",
    "                # ✅ valid subset만 예측\n",
    "                idx_s = np.where(va_s_mask)[0]\n",
    "                idx_u = np.where(~va_s_mask)[0]\n",
    "                if len(idx_s):\n",
    "                    px[idx_s] = px_s.predict(va_feat.iloc[idx_s]).values\n",
    "                    py[idx_s] = py_s.predict(va_feat.iloc[idx_s]).values\n",
    "                if len(idx_u):\n",
    "                    px[idx_u] = px_u.predict(va_feat.iloc[idx_u]).values\n",
    "                    py[idx_u] = py_u.predict(va_feat.iloc[idx_u]).values\n",
    "\n",
    "            finally:\n",
    "                for p in [path_sx, path_sy, path_ux, path_uy]:\n",
    "                    if os.path.exists(p):\n",
    "                        shutil.rmtree(p)\n",
    "\n",
    "        px, py = clip_xy(px, py)\n",
    "        oof[va_idx, 0] = px\n",
    "        oof[va_idx, 1] = py\n",
    "\n",
    "    y_true = data_merged[[\"end_x\", \"end_y\"]].values\n",
    "    score = euclidean_mean_distance(y_true, oof)\n",
    "    return score, oof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0143b39b",
   "metadata": {},
   "source": [
    "### 4) 후처리(postprocess) 함수: 예측점을 start 기준으로 보정(shrink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51afa048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_postprocess(df_feat: pd.DataFrame, pred_xy: np.ndarray,\n",
    "                      forward_scale: float, lateral_shrink: float):\n",
    "    \"\"\"\n",
    "    후처리 아이디어:\n",
    "    - pred_x는 start_x 기준으로 전진/후진 변위를 forward_scale만큼 스케일\n",
    "    - pred_y는 중앙선(GOAL_Y=34) 기준으로 lateral_shrink만큼 중앙으로 당김\n",
    "\n",
    "    주의: df_feat에 start_x가 반드시 있어야 함.\n",
    "    \"\"\"\n",
    "    start_x = df_feat[\"start_x\"].values\n",
    "\n",
    "    # x: start 기준 변위 스케일링\n",
    "    pred_x = start_x + (pred_xy[:, 0] - start_x) * forward_scale\n",
    "\n",
    "    # y: 중앙선 기준 shrink\n",
    "    pred_y = GOAL_Y + (pred_xy[:, 1] - GOAL_Y) * lateral_shrink\n",
    "\n",
    "    # 보정 후에도 경기장 범위로 클리핑\n",
    "    pred_x, pred_y = clip_xy(pred_x, pred_y)\n",
    "    return np.column_stack([pred_x, pred_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058df2cf",
   "metadata": {},
   "source": [
    "### 5) 메인 실행(STEP 1 ~ 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7962f32c",
   "metadata": {},
   "source": [
    "#### 하이퍼/프리셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7de637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_LIST   = [3, 5, 7, 10]\n",
    "N_SPLITS = 5\n",
    "\n",
    "TIME_LIMIT_PER_FOLD = None\n",
    "\n",
    "SEARCH_PRESETS_K = \"medium_quality\"  # k_prev 탐색(cheap)\n",
    "SEARCH_PRESETS   = \"good_quality\"    # 후보 비교/프루닝\n",
    "FINAL_PRESETS    = \"best_quality\"    # 최종 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb86c714",
   "metadata": {},
   "source": [
    "#### STEP 1: k_prev(k개의 과거 정보 이용) 선택 (cheap preset + OOF 캐시)\n",
    "**Pass가 시작되기 직전 몇 개(k개)의 과거 이벤트 정보를 피처로 사용할지(k_prev) 결정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7911cd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       17.67 GB / 31.59 GB (55.9%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== STEP1: Search k_prev=3 (cheap preset) ====\n",
      "[CACHE HIT] ksearch_k3_f7086118df8_rmse_rmse_cv5\n",
      "[k=3] OOF mean euclidean = 14.169963\n",
      "\n",
      "==== STEP1: Search k_prev=5 (cheap preset) ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold0\"\n",
      "Train Data Rows:    12324\n",
      "Train Data Columns: 73\n",
      "Label Column:       end_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    18095.02 MB\n",
      "\tTrain Data (Original)  Memory Usage: 14.09 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 56 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 11 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 10 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 56 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.1s = Fit runtime\n",
      "\t73 features in original data used to generate 73 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.80 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.16s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11091, Val Rows: 1233\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/17.6 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.2029\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/17.6 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.2413\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.61s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.6845\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.1s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-12.1702\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.44s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.602\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.32s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/17.1 GB\n",
      "\t-12.7335\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.16s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\xgboost\\core.py:729: UserWarning: [14:26:51] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "\t-12.3362\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.8 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-12.5522\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.23s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.2693\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.375, 'LightGBM': 0.167, 'NeuralNetTorch': 0.167, 'NeuralNetFastAI': 0.125, 'LightGBMXT': 0.083, 'XGBoost': 0.083}\n",
      "\t-12.0493\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 91.49s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 17870.2 rows/s (1233 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold0\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.89 GB / 31.59 GB (53.5%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold1\"\n",
      "Train Data Rows:    12382\n",
      "Train Data Columns: 73\n",
      "Label Column:       end_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17294.44 MB\n",
      "\tTrain Data (Original)  Memory Usage: 14.15 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 56 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 11 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 10 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 56 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.1s = Fit runtime\n",
      "\t73 features in original data used to generate 73 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.83 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.16s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11143, Val Rows: 1239\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.9 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.1912\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.9 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.1805\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.6898\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.52s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-12.1748\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.5342\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.22s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.6 GB\n",
      "No improvement since epoch 9: early stopping\n",
      "\t-12.7461\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.0s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-12.3688\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.7 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-12.5533\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.86s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.2824\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBM': 0.3, 'CatBoost': 0.3, 'LightGBMXT': 0.15, 'NeuralNetFastAI': 0.15, 'NeuralNetTorch': 0.1}\n",
      "\t-12.0587\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 83.61s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 20999.9 rows/s (1239 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold1\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       17.00 GB / 31.59 GB (53.8%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold2\"\n",
      "Train Data Rows:    12326\n",
      "Train Data Columns: 73\n",
      "Label Column:       end_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17406.39 MB\n",
      "\tTrain Data (Original)  Memory Usage: 14.09 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 56 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 11 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 10 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 56 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.1s = Fit runtime\n",
      "\t73 features in original data used to generate 73 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.80 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.16s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11093, Val Rows: 1233\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/17.0 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.0632\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/17.0 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.1575\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.4008\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.63s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-12.0474\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.4871\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.23s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.6 GB\n",
      "No improvement since epoch 8: early stopping\n",
      "\t-12.4457\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.15s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-12.224\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.7 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-12.5744\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.82s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.2864\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.478, 'NeuralNetFastAI': 0.304, 'LightGBMXT': 0.087, 'LightGBM': 0.087, 'RandomForestMSE': 0.043}\n",
      "\t-11.8873\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 67.05s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 14169.6 rows/s (1233 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold2\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       17.04 GB / 31.59 GB (54.0%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold3\"\n",
      "Train Data Rows:    12327\n",
      "Train Data Columns: 73\n",
      "Label Column:       end_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17452.14 MB\n",
      "\tTrain Data (Original)  Memory Usage: 14.09 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 56 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 11 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 10 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 56 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.2s = Fit runtime\n",
      "\t73 features in original data used to generate 73 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.80 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.19s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11094, Val Rows: 1233\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/17.0 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.1455\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.53s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/17.0 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.3184\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.6213\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.35s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-12.2236\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.6201\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.34s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.5 GB\n",
      "No improvement since epoch 8: early stopping\n",
      "\t-12.8501\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.06s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-12.341\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.7 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-12.7047\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.51s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.4435\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.533, 'CatBoost': 0.267, 'XGBoost': 0.133, 'NeuralNetFastAI': 0.067}\n",
      "\t-12.1017\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 75.5s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 26232.6 rows/s (1233 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold3\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       17.02 GB / 31.59 GB (53.9%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold4\"\n",
      "Train Data Rows:    12381\n",
      "Train Data Columns: 73\n",
      "Label Column:       end_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17421.14 MB\n",
      "\tTrain Data (Original)  Memory Usage: 14.15 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 56 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 11 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 10 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 56 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.2s = Fit runtime\n",
      "\t73 features in original data used to generate 73 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.83 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.17s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11142, Val Rows: 1239\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/17.0 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.1243\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/17.0 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.1443\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.4717\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.27s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-12.0005\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.4849\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.31s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.6 GB\n",
      "No improvement since epoch 8: early stopping\n",
      "\t-12.6149\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.05s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-12.1367\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.7 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-12.6003\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.8s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.9 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.2135\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.5, 'XGBoost': 0.25, 'NeuralNetFastAI': 0.188, 'NeuralNetTorch': 0.062}\n",
      "\t-11.9052\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 79.01s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 22528.9 rows/s (1239 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold4\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       17.02 GB / 31.59 GB (53.9%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold0\"\n",
      "Train Data Rows:    12324\n",
      "Train Data Columns: 73\n",
      "Label Column:       end_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17438.30 MB\n",
      "\tTrain Data (Original)  Memory Usage: 14.09 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 56 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 11 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 10 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 56 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.2s = Fit runtime\n",
      "\t73 features in original data used to generate 73 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.80 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.18s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11091, Val Rows: 1233\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/17.0 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.1065\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/17.0 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.2527\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.44s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.3635\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.16s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-13.0529\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.99s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.3026\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.21s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.6 GB\n",
      "No improvement since epoch 9: early stopping\n",
      "\t-13.7155\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.69s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-13.3931\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.7 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-14.3489\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.66s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.2936\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.545, 'LightGBMXT': 0.227, 'NeuralNetFastAI': 0.136, 'ExtraTreesMSE': 0.091}\n",
      "\t-12.9975\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 80.47s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 15803.6 rows/s (1233 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold0\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.93 GB / 31.59 GB (53.6%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold1\"\n",
      "Train Data Rows:    12382\n",
      "Train Data Columns: 73\n",
      "Label Column:       end_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17351.03 MB\n",
      "\tTrain Data (Original)  Memory Usage: 14.15 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 56 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 11 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 10 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 56 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.2s = Fit runtime\n",
      "\t73 features in original data used to generate 73 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.83 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.17s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11143, Val Rows: 1239\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.9 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.1653\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.9 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.4008\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.06s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.6092\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.97s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-13.1428\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.5319\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.23s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.5 GB\n",
      "\t-13.7915\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.86s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-13.3204\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.7 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-14.067\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.33s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.4865\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.44, 'LightGBMXT': 0.28, 'NeuralNetFastAI': 0.16, 'XGBoost': 0.08, 'NeuralNetTorch': 0.04}\n",
      "\t-13.0802\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 69.36s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 19508.0 rows/s (1239 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold1\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.95 GB / 31.59 GB (53.7%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold2\"\n",
      "Train Data Rows:    12326\n",
      "Train Data Columns: 73\n",
      "Label Column:       end_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17344.77 MB\n",
      "\tTrain Data (Original)  Memory Usage: 14.09 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 56 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 11 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 10 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 56 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.2s = Fit runtime\n",
      "\t73 features in original data used to generate 73 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.80 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.19s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11093, Val Rows: 1233\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.9 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.962\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.81s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.9 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.1788\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.0598\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.23s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-13.0031\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.1582\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.23s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.6 GB\n",
      "\t-13.6523\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.54s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-13.1331\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.7 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-13.7579\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.01s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.9 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.1046\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.66s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'RandomForestMSE': 0.286, 'LightGBMXT': 0.238, 'CatBoost': 0.19, 'NeuralNetFastAI': 0.19, 'NeuralNetTorch': 0.095}\n",
      "\t-12.8287\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 65.84s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 12947.2 rows/s (1233 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold2\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       17.05 GB / 31.59 GB (54.0%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold3\"\n",
      "Train Data Rows:    12327\n",
      "Train Data Columns: 73\n",
      "Label Column:       end_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17467.30 MB\n",
      "\tTrain Data (Original)  Memory Usage: 14.09 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 56 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 11 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 10 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 56 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.2s = Fit runtime\n",
      "\t73 features in original data used to generate 73 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.80 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.17s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11094, Val Rows: 1233\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/17.0 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.3387\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/17.0 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.4451\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.48\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.81s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-13.3343\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.5304\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.17s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.6 GB\n",
      "No improvement since epoch 8: early stopping\n",
      "\t-13.8371\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.65s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-13.5091\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.8 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-14.3326\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.73s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.9 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.3175\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.78s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBMLarge': 0.389, 'CatBoost': 0.278, 'LightGBMXT': 0.167, 'NeuralNetFastAI': 0.111, 'RandomForestMSE': 0.056}\n",
      "\t-13.2115\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 82.42s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 14252.4 rows/s (1233 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold3\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       17.03 GB / 31.59 GB (53.9%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold4\"\n",
      "Train Data Rows:    12381\n",
      "Train Data Columns: 73\n",
      "Label Column:       end_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17439.35 MB\n",
      "\tTrain Data (Original)  Memory Usage: 14.15 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 56 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 11 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 10 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 56 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.1s = Fit runtime\n",
      "\t73 features in original data used to generate 73 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.83 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.17s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11142, Val Rows: 1239\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/17.0 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.2729\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/17.0 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.325\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.95s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.4987\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.42s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-13.4231\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.5685\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.18s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.5 GB\n",
      "No improvement since epoch 8: early stopping\n",
      "\t-13.7715\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.84s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-13.4727\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.14s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.7 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-14.2038\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.78s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.3033\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.417, 'LightGBMLarge': 0.375, 'NeuralNetFastAI': 0.167, 'NeuralNetTorch': 0.042}\n",
      "\t-13.1592\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 72.61s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 21299.7 rows/s (1239 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold4\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.68 GB / 31.59 GB (52.8%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k=5] OOF mean euclidean = 14.280500\n",
      "\n",
      "==== STEP1: Search k_prev=7 (cheap preset) ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold0\"\n",
      "Train Data Rows:    12324\n",
      "Train Data Columns: 93\n",
      "Label Column:       end_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17088.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 18.61 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 72 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 15 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 14 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 72 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.2s = Fit runtime\n",
      "\t93 features in original data used to generate 93 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.35 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.24s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11091, Val Rows: 1233\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.7 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.265\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.6 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.2638\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.6881\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.75s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-12.1963\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.44s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.582\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.12s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.5 GB\n",
      "\t-12.8352\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.81s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-12.4206\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.71s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.7 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-12.8692\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.43s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.3545\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.462, 'NeuralNetFastAI': 0.231, 'LightGBM': 0.154, 'XGBoost': 0.154}\n",
      "\t-12.0779\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 90.19s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 21441.7 rows/s (1233 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold0\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.94 GB / 31.59 GB (53.6%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold1\"\n",
      "Train Data Rows:    12382\n",
      "Train Data Columns: 93\n",
      "Label Column:       end_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17363.42 MB\n",
      "\tTrain Data (Original)  Memory Usage: 18.70 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 72 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 15 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 14 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 72 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.2s = Fit runtime\n",
      "\t93 features in original data used to generate 93 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.39 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.23s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11143, Val Rows: 1239\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.9 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.1263\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.67s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.9 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.1692\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.53s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.6254\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.63s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-12.1528\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.36s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.5807\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.09s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.5 GB\n",
      "\t-12.9678\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.51s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-12.2641\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.45s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.6 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-12.7838\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.57s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.7 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.3611\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.36, 'CatBoost': 0.28, 'LightGBM': 0.16, 'XGBoost': 0.12, 'NeuralNetFastAI': 0.08}\n",
      "\t-12.048\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 95.04s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 19665.3 rows/s (1239 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold1\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.88 GB / 31.59 GB (53.4%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold2\"\n",
      "Train Data Rows:    12326\n",
      "Train Data Columns: 93\n",
      "Label Column:       end_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17293.21 MB\n",
      "\tTrain Data (Original)  Memory Usage: 18.62 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 72 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 15 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 14 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 72 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.2s = Fit runtime\n",
      "\t93 features in original data used to generate 93 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.35 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.22s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11093, Val Rows: 1233\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.9 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.0633\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.89s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.0532\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.4525\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.54s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-12.1069\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.78s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.5152\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.2s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.3 GB\n",
      "No improvement since epoch 5: early stopping\n",
      "\t-12.7168\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.89s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-12.1476\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.4 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-12.8678\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.36s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.6 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.2523\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.05s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBM': 0.364, 'LightGBMXT': 0.182, 'CatBoost': 0.182, 'NeuralNetFastAI': 0.136, 'XGBoost': 0.136}\n",
      "\t-11.9404\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 77.18s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 18669.3 rows/s (1233 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold2\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.77 GB / 31.59 GB (53.1%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold3\"\n",
      "Train Data Rows:    12327\n",
      "Train Data Columns: 93\n",
      "Label Column:       end_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17174.51 MB\n",
      "\tTrain Data (Original)  Memory Usage: 18.62 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 72 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 15 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 14 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 72 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.2s = Fit runtime\n",
      "\t93 features in original data used to generate 93 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.35 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.23s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11094, Val Rows: 1233\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.7 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.2428\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.66s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.3248\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.42s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.6508\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.06s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-12.198\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.6213\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.08s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.4 GB\n",
      "No improvement since epoch 9: early stopping\n",
      "\t-12.8404\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.42s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-12.4417\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.6 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-12.8176\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.58s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.7 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.396\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.474, 'LightGBMXT': 0.211, 'NeuralNetFastAI': 0.158, 'LightGBM': 0.105, 'XGBoost': 0.053}\n",
      "\t-12.131\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 99.94s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 19571.9 rows/s (1233 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold3\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.80 GB / 31.59 GB (53.2%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold4\"\n",
      "Train Data Rows:    12381\n",
      "Train Data Columns: 93\n",
      "Label Column:       end_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17203.65 MB\n",
      "\tTrain Data (Original)  Memory Usage: 18.70 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 72 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 15 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 14 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 72 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.2s = Fit runtime\n",
      "\t93 features in original data used to generate 93 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.39 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.23s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11142, Val Rows: 1239\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.0758\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.1336\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.4246\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.97s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-12.0085\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.85s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.4544\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.05s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.5 GB\n",
      "No improvement since epoch 7: early stopping\n",
      "\t-12.73\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.59s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-12.2521\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.7 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-12.5726\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.18s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.2705\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.44, 'LightGBMXT': 0.24, 'NeuralNetFastAI': 0.12, 'NeuralNetTorch': 0.12, 'LightGBM': 0.08}\n",
      "\t-11.9039\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 102.07s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 16770.9 rows/s (1239 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold4\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.93 GB / 31.59 GB (53.6%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold0\"\n",
      "Train Data Rows:    12324\n",
      "Train Data Columns: 93\n",
      "Label Column:       end_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17334.41 MB\n",
      "\tTrain Data (Original)  Memory Usage: 18.61 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 72 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 15 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 14 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 72 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.2s = Fit runtime\n",
      "\t93 features in original data used to generate 93 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.35 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.23s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11091, Val Rows: 1233\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.9 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.1995\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.9 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.3633\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.3707\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.52s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-13.1626\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.82s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.393\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.15s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.4 GB\n",
      "No improvement since epoch 9: early stopping\n",
      "\t-13.9174\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.22s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-13.4244\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.33s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.5 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-14.5983\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.35s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.6 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.3577\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.435, 'LightGBMXT': 0.261, 'RandomForestMSE': 0.13, 'NeuralNetFastAI': 0.13, 'LightGBMLarge': 0.043}\n",
      "\t-13.079\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 101.96s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 10824.6 rows/s (1233 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold0\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.77 GB / 31.59 GB (53.1%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold1\"\n",
      "Train Data Rows:    12382\n",
      "Train Data Columns: 93\n",
      "Label Column:       end_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17180.59 MB\n",
      "\tTrain Data (Original)  Memory Usage: 18.70 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 72 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 15 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 14 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 72 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.2s = Fit runtime\n",
      "\t93 features in original data used to generate 93 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.39 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.23s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11143, Val Rows: 1239\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.3131\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.5156\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.6658\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.33s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-13.2168\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.5599\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.04s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.3 GB\n",
      "\t-14.1024\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.97s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-13.4617\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.5s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.5 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-14.3478\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.8s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.6 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.5272\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.83s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.708, 'NeuralNetFastAI': 0.167, 'LightGBMXT': 0.083, 'XGBoost': 0.042}\n",
      "\t-13.1684\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 91.29s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 21243.8 rows/s (1239 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold1\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.74 GB / 31.59 GB (53.0%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold2\"\n",
      "Train Data Rows:    12326\n",
      "Train Data Columns: 93\n",
      "Label Column:       end_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17148.74 MB\n",
      "\tTrain Data (Original)  Memory Usage: 18.62 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 72 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 15 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 14 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 72 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.2s = Fit runtime\n",
      "\t93 features in original data used to generate 93 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.35 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.22s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11093, Val Rows: 1233\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.7 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.1173\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.7 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.1605\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.1683\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.46s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-13.0008\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.2275\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.98s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.4 GB\n",
      "No improvement since epoch 8: early stopping\n",
      "\t-13.8423\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.14s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-13.1587\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.45s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.6 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-13.9394\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.4s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.7 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.0566\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.2s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.5, 'LightGBMLarge': 0.318, 'RandomForestMSE': 0.091, 'NeuralNetFastAI': 0.091}\n",
      "\t-12.9241\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 85.35s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 12196.7 rows/s (1233 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold2\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.81 GB / 31.59 GB (53.2%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold3\"\n",
      "Train Data Rows:    12327\n",
      "Train Data Columns: 93\n",
      "Label Column:       end_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17224.36 MB\n",
      "\tTrain Data (Original)  Memory Usage: 18.62 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 72 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 15 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 14 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 72 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.2s = Fit runtime\n",
      "\t93 features in original data used to generate 93 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.35 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.22s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11094, Val Rows: 1233\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.4283\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.4613\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.5503\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.73s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-13.3848\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.5391\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.06s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.4 GB\n",
      "\t-13.999\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.37s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-13.4413\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.3s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.6 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-14.6295\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.26s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.7 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.3988\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.333, 'NeuralNetFastAI': 0.267, 'LightGBMLarge': 0.267, 'RandomForestMSE': 0.067, 'XGBoost': 0.067}\n",
      "\t-13.1838\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 95.64s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 12441.4 rows/s (1233 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold3\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.89 GB / 31.59 GB (53.5%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold4\"\n",
      "Train Data Rows:    12381\n",
      "Train Data Columns: 93\n",
      "Label Column:       end_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17296.61 MB\n",
      "\tTrain Data (Original)  Memory Usage: 18.70 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 72 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 15 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 14 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 72 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.2s = Fit runtime\n",
      "\t93 features in original data used to generate 93 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.39 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.24s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11142, Val Rows: 1239\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.9 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.4033\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.9 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.4964\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.5834\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.79s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-13.4112\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.6212\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.99s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.4 GB\n",
      "No improvement since epoch 8: early stopping\n",
      "\t-14.0623\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.79s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-13.6215\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.61s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.6 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-14.3452\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.15s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.7 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.4797\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.333, 'LightGBMXT': 0.267, 'LightGBMLarge': 0.267, 'NeuralNetFastAI': 0.133}\n",
      "\t-13.2989\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 89.43s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 23372.7 rows/s (1239 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold4\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.79 GB / 31.59 GB (53.2%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k=7] OOF mean euclidean = 14.351664\n",
      "\n",
      "==== STEP1: Search k_prev=10 (cheap preset) ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold0\"\n",
      "Train Data Rows:    12324\n",
      "Train Data Columns: 123\n",
      "Label Column:       end_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17209.80 MB\n",
      "\tTrain Data (Original)  Memory Usage: 25.39 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 96 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 21 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 20 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 96 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.3s = Fit runtime\n",
      "\t123 features in original data used to generate 123 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 9.68 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.32s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11091, Val Rows: 1233\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.281\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.3297\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.74s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.7334\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.8s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-12.2077\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.5891\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.28s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.4 GB\n",
      "No improvement since epoch 5: early stopping\n",
      "\t-13.3344\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.8s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-12.4921\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.61s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.5 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-13.0618\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.7s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.7 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.4099\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.562, 'LightGBMXT': 0.312, 'LightGBM': 0.125}\n",
      "\t-12.158\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 120.99s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 44031.4 rows/s (1233 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold0\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.83 GB / 31.59 GB (53.3%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold1\"\n",
      "Train Data Rows:    12382\n",
      "Train Data Columns: 123\n",
      "Label Column:       end_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17248.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 25.51 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 96 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 21 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 20 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 96 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.3s = Fit runtime\n",
      "\t123 features in original data used to generate 123 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 9.73 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.29s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11143, Val Rows: 1239\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.2177\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.2521\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.6557\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.72s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-12.2915\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.5858\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.29s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.4 GB\n",
      "No improvement since epoch 9: early stopping\n",
      "\t-13.099\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.15s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-12.2636\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.58s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.6 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-12.9192\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.86s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.7 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.2278\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBMLarge': 0.308, 'LightGBMXT': 0.231, 'XGBoost': 0.231, 'NeuralNetFastAI': 0.154, 'CatBoost': 0.077}\n",
      "\t-12.0819\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 124.81s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 15780.9 rows/s (1239 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold1\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.82 GB / 31.59 GB (53.3%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold2\"\n",
      "Train Data Rows:    12326\n",
      "Train Data Columns: 123\n",
      "Label Column:       end_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17246.82 MB\n",
      "\tTrain Data (Original)  Memory Usage: 25.40 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 96 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 21 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 20 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 96 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.3s = Fit runtime\n",
      "\t123 features in original data used to generate 123 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 9.68 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.31s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11093, Val Rows: 1233\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.2042\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.2019\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.5025\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.38s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-12.2081\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.92s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.619\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.27s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.4 GB\n",
      "No improvement since epoch 8: early stopping\n",
      "\t-12.982\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.68s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-12.1733\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.95s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.5 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-12.9428\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.75s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.6 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.2956\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'XGBoost': 0.4, 'CatBoost': 0.24, 'NeuralNetFastAI': 0.2, 'LightGBM': 0.12, 'LightGBMXT': 0.04}\n",
      "\t-12.0032\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 101.97s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 14504.8 rows/s (1233 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold2\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.79 GB / 31.59 GB (53.2%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold3\"\n",
      "Train Data Rows:    12327\n",
      "Train Data Columns: 123\n",
      "Label Column:       end_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17217.26 MB\n",
      "\tTrain Data (Original)  Memory Usage: 25.40 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 96 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 21 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 20 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 96 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.2s = Fit runtime\n",
      "\t123 features in original data used to generate 123 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 9.68 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.26s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11094, Val Rows: 1233\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.3314\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.4429\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.6706\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.06s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-12.251\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.6381\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.39s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.3 GB\n",
      "No improvement since epoch 9: early stopping\n",
      "\t-13.1712\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.9s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-12.4508\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.73s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.5 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-12.8743\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.78s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.6 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.4502\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.524, 'XGBoost': 0.19, 'NeuralNetFastAI': 0.143, 'LightGBMXT': 0.095, 'NeuralNetTorch': 0.048}\n",
      "\t-12.1784\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 105.29s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 14943.3 rows/s (1233 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold3\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.77 GB / 31.59 GB (53.1%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold4\"\n",
      "Train Data Rows:    12381\n",
      "Train Data Columns: 123\n",
      "Label Column:       end_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17176.56 MB\n",
      "\tTrain Data (Original)  Memory Usage: 25.51 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 96 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 21 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 20 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 96 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.2s = Fit runtime\n",
      "\t123 features in original data used to generate 123 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 9.73 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.27s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11142, Val Rows: 1239\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.7 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.1437\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.7 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.1621\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.4949\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.02s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-11.9932\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-12.5109\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.24s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.3 GB\n",
      "No improvement since epoch 9: early stopping\n",
      "\t-12.9279\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.42s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-12.3764\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.44s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.5 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-13.0157\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.74s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.7 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.2147\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.667, 'LightGBM': 0.19, 'NeuralNetFastAI': 0.143}\n",
      "\t-11.9395\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 112.92s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 24781.7 rows/s (1239 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_x_fold4\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.83 GB / 31.59 GB (53.3%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold0\"\n",
      "Train Data Rows:    12324\n",
      "Train Data Columns: 123\n",
      "Label Column:       end_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17252.97 MB\n",
      "\tTrain Data (Original)  Memory Usage: 25.39 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 96 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 21 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 20 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 96 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.3s = Fit runtime\n",
      "\t123 features in original data used to generate 123 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 9.68 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.29s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11091, Val Rows: 1233\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.2062\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.4445\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.86s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.3891\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.98s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-13.1859\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.4729\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.21s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.4 GB\n",
      "No improvement since epoch 9: early stopping\n",
      "\t-14.1948\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.07s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-13.4368\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.44s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.5 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-14.8641\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.84s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.6 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.2365\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.435, 'LightGBMLarge': 0.348, 'NeuralNetFastAI': 0.13, 'LightGBMXT': 0.087}\n",
      "\t-13.0753\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 90.76s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 21632.4 rows/s (1233 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold0\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.76 GB / 31.59 GB (53.1%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold1\"\n",
      "Train Data Rows:    12382\n",
      "Train Data Columns: 123\n",
      "Label Column:       end_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17184.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 25.51 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 96 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 21 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 20 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 96 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.3s = Fit runtime\n",
      "\t123 features in original data used to generate 123 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 9.73 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.29s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11143, Val Rows: 1239\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.8 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.3185\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.7 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.4845\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.6803\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.41s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-13.2424\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.67s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.6141\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.18s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.3 GB\n",
      "No improvement since epoch 8: early stopping\n",
      "\t-14.1273\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.59s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-13.5167\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.41s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.4 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-14.3248\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.37s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.5 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.4314\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.78s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.571, 'LightGBMLarge': 0.214, 'LightGBMXT': 0.071, 'NeuralNetFastAI': 0.071, 'NeuralNetTorch': 0.071}\n",
      "\t-13.1963\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 83.63s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 15680.9 rows/s (1239 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold1\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.71 GB / 31.59 GB (52.9%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold2\"\n",
      "Train Data Rows:    12326\n",
      "Train Data Columns: 123\n",
      "Label Column:       end_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17117.15 MB\n",
      "\tTrain Data (Original)  Memory Usage: 25.40 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 96 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 21 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 20 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 96 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.3s = Fit runtime\n",
      "\t123 features in original data used to generate 123 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 9.68 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.29s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11093, Val Rows: 1233\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.7 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.0404\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.7 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.1863\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.23s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.185\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.52s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-13.0016\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.51s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.2905\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.18s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.3 GB\n",
      "No improvement since epoch 6: early stopping\n",
      "\t-14.1749\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.78s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-13.1886\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.34s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.5 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-14.3054\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.5s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.5 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.1748\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.03s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.52, 'LightGBMXT': 0.24, 'RandomForestMSE': 0.16, 'LightGBMLarge': 0.08}\n",
      "\t-12.9581\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 86.06s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 17612.1 rows/s (1233 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold2\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.69 GB / 31.59 GB (52.8%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold3\"\n",
      "Train Data Rows:    12327\n",
      "Train Data Columns: 123\n",
      "Label Column:       end_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17108.79 MB\n",
      "\tTrain Data (Original)  Memory Usage: 25.40 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 96 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 21 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 20 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 96 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.3s = Fit runtime\n",
      "\t123 features in original data used to generate 123 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 9.68 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.29s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11094, Val Rows: 1233\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.7 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.3805\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.7 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.48\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.5565\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.96s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-13.4366\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.6618\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.25s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.3 GB\n",
      "No improvement since epoch 9: early stopping\n",
      "\t-14.2919\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.1s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-13.5185\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.31s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.5 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-14.8974\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.97s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.5 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.4083\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.35, 'LightGBMLarge': 0.3, 'RandomForestMSE': 0.15, 'CatBoost': 0.1, 'NeuralNetFastAI': 0.1}\n",
      "\t-13.2852\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 90.15s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 10815.0 rows/s (1233 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold3\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       16.92 GB / 31.59 GB (53.6%)\n",
      "Disk Space Avail:   25.93 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold4\"\n",
      "Train Data Rows:    12381\n",
      "Train Data Columns: 123\n",
      "Label Column:       end_y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17337.44 MB\n",
      "\tTrain Data (Original)  Memory Usage: 25.51 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 96 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) : 21 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 20 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 96 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.3s = Fit runtime\n",
      "\t123 features in original data used to generate 123 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 9.73 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.28s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 11142, Val Rows: 1239\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.9 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.466\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.9 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.4886\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.5608\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.83s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-13.4996\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tFitting with cpus=12, gpus=1\n",
      "\t-13.7064\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.1s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.4 GB\n",
      "No improvement since epoch 6: early stopping\n",
      "\t-14.3173\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.57s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=6, gpus=1\n",
      "\t-13.5942\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.37s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.0/16.5 GB\n",
      "d:\\공모전\\스포츠\\venv\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-14.6073\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.9s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=6, gpus=1, mem=0.1/16.6 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-13.5351\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'RandomForestMSE': 0.28, 'LightGBMXT': 0.24, 'LightGBM': 0.2, 'CatBoost': 0.16, 'NeuralNetFastAI': 0.08, 'NeuralNetTorch': 0.04}\n",
      "\t-13.3875\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 98.51s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 10346.9 rows/s (1239 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"d:\\공모전\\스포츠\\ag_tmp\\end_y_fold4\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k=10] OOF mean euclidean = 14.394723\n"
     ]
    }
   ],
   "source": [
    "best_k = {\"k_prev\": None, \"oof\": float(\"inf\"), \"oof_pred\": None}\n",
    "\n",
    "# k_prev별로 OOF score를 계산 -> 가장 작은 score를 best로 선택\n",
    "for k_prev in K_LIST:\n",
    "    print(f\"\\n==== STEP1: Search k_prev={k_prev} (cheap preset) ====\")\n",
    "\n",
    "    # k_prev별 데이터 로드\n",
    "    X_train, y_train, _ = load_pack(k_prev)\n",
    "\n",
    "    # merged data + 그룹 + feature 컬럼\n",
    "    data, yx, yy, groups, feat_cols = prepare_data(X_train, y_train)\n",
    "\n",
    "    # ✅ k 탐색도 캐시 사용(자주 재실행하니까)\n",
    "    key = make_oof_key(k_prev, feat_cols, \"rmse\", \"rmse\", N_SPLITS, tag=\"ksearch\")\n",
    "    cached = load_oof_cache(key)\n",
    "\n",
    "    if cached is not None:\n",
    "        score, oof_pred = cached\n",
    "        print(f\"[CACHE HIT] {key}\")\n",
    "        \n",
    "    else:\n",
    "        score, oof_pred = ag_cv_score_xy(\n",
    "            data_merged=data,\n",
    "            feat_cols=feat_cols,\n",
    "            groups=groups,\n",
    "            x_metric=\"rmse\",\n",
    "            y_metric=\"rmse\",\n",
    "            n_splits=N_SPLITS,\n",
    "            presets=SEARCH_PRESETS_K,\n",
    "            time_limit=TIME_LIMIT_PER_FOLD,\n",
    "            num_gpus=NUM_GPUS,\n",
    "            fold_fitting_strategy=FOLD_FITTING_STRATEGY,\n",
    "        )\n",
    "        save_oof_cache(key, (score, oof_pred))\n",
    "\n",
    "    print(f\"[k={k_prev}] OOF mean euclidean = {score:.6f}\")\n",
    "    if score < best_k[\"oof\"]:\n",
    "        best_k.update({\"k_prev\": k_prev, \"oof\": score, \"oof_pred\": oof_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc36835f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEST k_prev: 3 baseline OOF: 14.169962815260774\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBEST k_prev:\", best_k[\"k_prev\"], \"baseline OOF:\", best_k[\"oof\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3798356",
   "metadata": {},
   "source": [
    "#### STEP 2: 선택된 k_prev에서 pruning (importance top TOP_N) : 피처 선택\n",
    "**STEP 1에서 선택된 피처 셋에서 모델에 불필요하거나 노이즈가 많은 피처를 제거**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a58558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선택된 k_prev pack 로드\n",
    "X_train, y_train, X_test = load_pack(k_prev)\n",
    "data, yx, yy, groups, feat_cols = prepare_data(X_train, y_train)\n",
    "\n",
    "TOP_N = min(200, len(feat_cols))\n",
    "print(f\"\\n==== STEP2: Pruning via CV-train importance (top {TOP_N}) ====\")\n",
    "\n",
    "pruned_features, imp_all = ag_prune_features_cv(\n",
    "    data_merged=data,\n",
    "    feat_cols=feat_cols,\n",
    "    groups=groups,\n",
    "    n_splits=N_SPLITS,\n",
    "    top_n=TOP_N,\n",
    "    eval_metric=\"rmse\",\n",
    "    presets=SEARCH_PRESETS,\n",
    "    time_limit=TIME_LIMIT_PER_FOLD,\n",
    "    num_gpus=NUM_GPUS,\n",
    "    save_root=TMP_DIR,\n",
    "    fold_fitting_strategy=FOLD_FITTING_STRATEGY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3b5b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 20 pruned features:\")\n",
    "print(imp_all.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd838ca",
   "metadata": {},
   "source": [
    "#### STEP 3: metric(=loss 성격) 분리 비교 (baseline vs pruned) : “모델 후보” 평가\n",
    "**피처 셋과 AutoGluon 내부 metric(end_x / end_y 각각 rmse vs mae 중 무엇이 더 유리한지)의 최적 조합**  \n",
    " 후보: (x=rmse, y=rmse) vs (x=rmse, y=mae)\n",
    " - baseline feature + (rmse, rmse)\n",
    " - baseline feature + (rmse, mae)\n",
    " - pruned feature + (rmse, rmse)\n",
    " - pruned feature + (rmse, mae)  \n",
    " => 각각 OOF score를 구해서 최솟값을 best로 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf02ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==== STEP3: Candidates (no-branch yet) with OOF cache ====\")\n",
    "\n",
    "candidates = []\n",
    "\n",
    "def eval_candidate(tag_name, fcols, x_metric, y_metric):\n",
    "    \"\"\"\n",
    "    후보 1개를 평가(OOF score + OOF pred)하고 candidates에 넣기\n",
    "    - ✅ OOF 캐시 사용으로 중복 학습 제거\n",
    "    \"\"\"\n",
    "    key = make_oof_key(k_prev, fcols, x_metric, y_metric, N_SPLITS, tag=\"cand\")\n",
    "    cached = load_oof_cache(key)\n",
    "    if cached is not None:\n",
    "        score, oof_pred = cached\n",
    "        print(f\"[CACHE HIT] {tag_name} -> {score}\")\n",
    "    else:\n",
    "        score, oof_pred = ag_cv_score_xy(\n",
    "            data_merged=data,\n",
    "            feat_cols=fcols,\n",
    "            groups=groups,\n",
    "            x_metric=x_metric,\n",
    "            y_metric=y_metric,\n",
    "            n_splits=N_SPLITS,\n",
    "            presets=SEARCH_PRESETS,\n",
    "            time_limit=TIME_LIMIT_PER_FOLD,\n",
    "            num_gpus=NUM_GPUS,\n",
    "            fold_fitting_strategy=FOLD_FITTING_STRATEGY,\n",
    "        )\n",
    "\n",
    "        save_oof_cache(key, (score, oof_pred))\n",
    "        print(f\"[TRAINED]   {tag_name} -> {score:.6f}\")\n",
    "\n",
    "    candidates.append((tag_name, fcols, x_metric, y_metric, score, oof_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0cb83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "eval_candidate(\"baseline_rmse_rmse\", feat_cols,        \"rmse\", \"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc76ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_candidate(\"baseline_rmse_mae\",  feat_cols,        \"rmse\", \"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504ebdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruned\n",
    "eval_candidate(\"pruned_rmse_rmse\",   pruned_features,  \"rmse\", \"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ae36be",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_candidate(\"pruned_rmse_mae\",    pruned_features,  \"rmse\", \"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c971f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = min(candidates, key=lambda x: x[4])\n",
    "best_name, best_fcols, best_xm, best_ym, base_score, base_oof = best\n",
    "print(\"\\nBest candidate (no-branch yet):\", best_name, \"score:\", base_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2559e9",
   "metadata": {},
   "source": [
    "#### STEP 4: result_name 분기 모델 비교 (best candidate 기준)\n",
    "**Pass 결과(Successful vs Unsuccessful)별로 모델을 분리하여 학습하는 Branching 전략이 단일 모델보다 성능이 좋은지 확인**  \n",
    "분기 모델이 더 좋으면 use_branch=True, 아니면 base 후보 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f3a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==== STEP4: Branching check ====\")\n",
    "\n",
    "use_branch = False\n",
    "branch_score = float(\"inf\")\n",
    "branch_oof = None\n",
    "\n",
    "# data에 result_name 컬럼이 있는 경우에만 branching을 시도\n",
    "if \"result_name\" in data.columns:\n",
    "    # branching은 구조가 달라 캐시 키를 별도로 둠(원하면 캐시 가능)\n",
    "    branch_score, branch_oof = ag_cv_score_xy_branch(\n",
    "        data_merged=data,\n",
    "        feat_cols=best_fcols,\n",
    "        groups=groups,\n",
    "        x_metric=best_xm,\n",
    "        y_metric=best_ym,\n",
    "        result_col=\"result_name\",\n",
    "        n_splits=N_SPLITS,\n",
    "        presets=SEARCH_PRESETS,\n",
    "        time_limit=TIME_LIMIT_PER_FOLD,\n",
    "        min_side=50,\n",
    "        num_gpus=NUM_GPUS,\n",
    "        fold_fitting_strategy=FOLD_FITTING_STRATEGY,\n",
    "    )\n",
    "\n",
    "    print(\"Base best score:\", base_score)\n",
    "    print(\"Branch score  :\", branch_score)\n",
    "\n",
    "    use_branch = branch_score < base_score\n",
    "    print(\"Use branching?\", use_branch)\n",
    "else:\n",
    "    print(\"[INFO] data has no result_name -> skip branching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275bb1b2",
   "metadata": {},
   "source": [
    "#### STEP 5: 후처리 튜닝 (OOF 기반 grid search)\n",
    "**예측 좌표를 경기장의 특성에 맞게 보정하여 성능을 미세 조정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bd77fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==== STEP5: Postprocess tuning ====\")\n",
    "\n",
    "# 분기 사용 여부에 따라 사용할 OOF를 선택\n",
    "if use_branch:\n",
    "    oof_use = branch_oof\n",
    "    base_ref = branch_score\n",
    "else:\n",
    "    oof_use = base_oof\n",
    "    base_ref = base_score\n",
    "\n",
    "# grid 탐색 범위\n",
    "grid_forward = [0.90, 0.95, 1.00, 1.05]\n",
    "grid_lateral = [0.80, 0.90, 1.00]\n",
    "\n",
    "y_true = data[[\"end_x\", \"end_y\"]].values\n",
    "\n",
    "# 초기값은 “후처리 안 함”\n",
    "best_pp = {\"forward_scale\": 1.0, \"lateral_shrink\": 1.0, \"score\": base_ref}\n",
    "\n",
    "# grid search: OOF에 후처리 적용 후 euclidean 최소 조합 찾기\n",
    "for fs in grid_forward:\n",
    "    for ls in grid_lateral:\n",
    "        adj = apply_postprocess(data, oof_use, fs, ls)\n",
    "        s = euclidean_mean_distance(y_true, adj)\n",
    "        if s < best_pp[\"score\"]:\n",
    "            best_pp = {\"forward_scale\": fs, \"lateral_shrink\": ls, \"score\": s}\n",
    "\n",
    "print(\"Best postprocess:\", best_pp)\n",
    "\n",
    "# 후처리 적용이 실제로 이득이면 사용\n",
    "use_postprocess = best_pp[\"score\"] < base_ref\n",
    "print(\"Use postprocess?\", use_postprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a2316b",
   "metadata": {},
   "source": [
    "#### STEP 6: 최종 학습(전체 데이터) + 저장 + submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df34f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = {\n",
    "    \"k_prev\": int(k_prev),\n",
    "    \"feature_set\": best_name,\n",
    "    \"x_metric\": best_xm,\n",
    "    \"y_metric\": best_ym,\n",
    "    \"use_branch\": bool(use_branch),\n",
    "    \"use_postprocess\": bool(use_postprocess),\n",
    "    \"postprocess\": best_pp if use_postprocess else {\"forward_scale\": 1.0, \"lateral_shrink\": 1.0},\n",
    "    \"presets_k_search\": SEARCH_PRESETS_K,\n",
    "    \"presets_search\": SEARCH_PRESETS,\n",
    "    \"presets_final\": FINAL_PRESETS,\n",
    "    \"fold_fitting_strategy\": FOLD_FITTING_STRATEGY,\n",
    "    \"num_gpus\": int(NUM_GPUS),\n",
    "}\n",
    "print(\"\\nFINAL CONFIG:\", final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c0e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) 최종 train 준비 (+ 전처리 일관 적용)\n",
    "train_full_x = fill_object_missing(data[best_fcols + [\"end_x\"]].copy(), cols=best_fcols)\n",
    "train_full_y = fill_object_missing(data[best_fcols + [\"end_y\"]].copy(), cols=best_fcols)\n",
    "\n",
    "# ✅ 최종학습도 object 결측 일관 처리\n",
    "train_full_x = fill_object_missing(train_full_x, cols=best_fcols)\n",
    "train_full_y = fill_object_missing(train_full_y, cols=best_fcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4122cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) 테스트 준비 (+ 전처리)\n",
    "Xt = fill_object_missing(X_test[best_fcols].copy(), cols=best_fcols)\n",
    "\n",
    "# ✅ 테스트에 result_name 없으면 branching 불가 -> 자동 해제\n",
    "if use_branch and (\"result_name\" not in X_test.columns):\n",
    "    print(\"[WARN] X_test has no result_name. Disable branching.\")\n",
    "    use_branch = False\n",
    "    final[\"use_branch\"] = False\n",
    "\n",
    "def fit_kwargs_final():\n",
    "    \"\"\"최종 .fit()에 넘길 kwargs를 None 없이 안전하게 구성.\"\"\"\n",
    "    kw = dict(\n",
    "        presets=FINAL_PRESETS,\n",
    "        ag_args_fit={\"num_gpus\": int(NUM_GPUS)},\n",
    "    )\n",
    "    if FOLD_FITTING_STRATEGY is not None:\n",
    "        kw[\"ag_args_ensemble\"] = {\"fold_fitting_strategy\": FOLD_FITTING_STRATEGY}\n",
    "    return kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dc8631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) 최종 학습 및 예측\n",
    "if not use_branch:\n",
    "    # 단일 모델(분기 없음): end_x predictor 1개 + end_y predictor 1개\n",
    "    px_pred = TabularPredictor(\n",
    "        label=\"end_x\", problem_type=\"regression\", eval_metric=best_xm,\n",
    "        path=os.path.join(MODEL_DIR, \"predictor_endx\"),\n",
    "    ).fit(train_data=train_full_x, **fit_kwargs_final())\n",
    "\n",
    "    py_pred = TabularPredictor(\n",
    "        label=\"end_y\", problem_type=\"regression\", eval_metric=best_ym,\n",
    "        path=os.path.join(MODEL_DIR, \"predictor_endy\"),\n",
    "    ).fit(train_data=train_full_y, **fit_kwargs_final())\n",
    "\n",
    "    px = px_pred.predict(Xt).values\n",
    "    py = py_pred.predict(Xt).values\n",
    "\n",
    "else:\n",
    "    # 분기 모델: 성공/실패 각각 end_x/end_y predictor (총 4개)\n",
    "    is_s = data[\"result_name\"].astype(str).eq(\"Successful\")\n",
    "    is_u = ~is_s\n",
    "\n",
    "    # fallback: 한쪽이 너무 작으면(안전장치) 전체 데이터로 단일 모델 학습\n",
    "    if is_s.sum() < 50 or is_u.sum() < 50:\n",
    "        print(\"[WARN] One side too small -> fallback to single model\")\n",
    "\n",
    "        px_all = TabularPredictor(\n",
    "            label=\"end_x\", problem_type=\"regression\", eval_metric=best_xm,\n",
    "            path=os.path.join(MODEL_DIR, \"predictor_endx_all\"),\n",
    "        ).fit(train_data=train_full_x, **fit_kwargs_final())\n",
    "\n",
    "        py_all = TabularPredictor(\n",
    "            label=\"end_y\", problem_type=\"regression\", eval_metric=best_ym,\n",
    "            path=os.path.join(MODEL_DIR, \"predictor_endy_all\"),\n",
    "        ).fit(train_data=train_full_y, **fit_kwargs_final())\n",
    "\n",
    "        px = px_all.predict(Xt).values\n",
    "        py = py_all.predict(Xt).values\n",
    "\n",
    "    else:\n",
    "        # 학습 데이터도 전처리 일관 유지\n",
    "        tr_s_x = fill_object_missing(data.loc[is_s, best_fcols + [\"end_x\"]].copy(), cols=best_fcols)\n",
    "        tr_s_y = fill_object_missing(data.loc[is_s, best_fcols + [\"end_y\"]].copy(), cols=best_fcols)\n",
    "        tr_u_x = fill_object_missing(data.loc[is_u, best_fcols + [\"end_x\"]].copy(), cols=best_fcols)\n",
    "        tr_u_y = fill_object_missing(data.loc[is_u, best_fcols + [\"end_y\"]].copy(), cols=best_fcols)\n",
    "\n",
    "        # 성공 모델\n",
    "        px_s = TabularPredictor(\n",
    "            label=\"end_x\", problem_type=\"regression\", eval_metric=best_xm,\n",
    "            path=os.path.join(MODEL_DIR, \"predictor_endx_success\"),\n",
    "        ).fit(train_data=tr_s_x, **fit_kwargs_final())\n",
    "\n",
    "        py_s = TabularPredictor(\n",
    "            label=\"end_y\", problem_type=\"regression\", eval_metric=best_ym,\n",
    "            path=os.path.join(MODEL_DIR, \"predictor_endy_success\"),\n",
    "        ).fit(train_data=tr_s_y, **fit_kwargs_final())\n",
    "\n",
    "        # 실패 모델\n",
    "        px_u = TabularPredictor(\n",
    "            label=\"end_x\", problem_type=\"regression\", eval_metric=best_xm,\n",
    "            path=os.path.join(MODEL_DIR, \"predictor_endx_unsuccess\"),\n",
    "        ).fit(train_data=tr_u_x, **fit_kwargs_final())\n",
    "\n",
    "        py_u = TabularPredictor(\n",
    "            label=\"end_y\", problem_type=\"regression\", eval_metric=best_ym,\n",
    "            path=os.path.join(MODEL_DIR, \"predictor_endy_unsuccess\"),\n",
    "        ).fit(train_data=tr_u_y, **fit_kwargs_final())\n",
    "\n",
    "        # 테스트 라우팅 (테스트에 result_name 있어야 여기까지 옴)\n",
    "        is_s_test = X_test[\"result_name\"].astype(str).eq(\"Successful\").values\n",
    "\n",
    "        # 예측 (전체를 예측한 뒤 라우팅 — 여기서는 최종 1회라서 단순 유지)\n",
    "        pred_s_x = px_s.predict(Xt).values\n",
    "        pred_s_y = py_s.predict(Xt).values\n",
    "        pred_u_x = px_u.predict(Xt).values\n",
    "        pred_u_y = py_u.predict(Xt).values\n",
    "\n",
    "        px = np.where(is_s_test, pred_s_x, pred_u_x)\n",
    "        py = np.where(is_s_test, pred_s_y, pred_u_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18274a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) 클리핑 + (선택) 후처리\n",
    "px, py = clip_xy(px, py)\n",
    "pred = np.column_stack([px, py])\n",
    "\n",
    "if use_postprocess:\n",
    "    pred = apply_postprocess(\n",
    "        df_feat=X_test,  # start_x 필요\n",
    "        pred_xy=pred,\n",
    "        forward_scale=best_pp[\"forward_scale\"],\n",
    "        lateral_shrink=best_pp[\"lateral_shrink\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724bbc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) submission 저장\n",
    "sub = pd.DataFrame({\n",
    "    \"game_episode\": X_test[\"game_episode\"].values,\n",
    "    \"end_x\": pred[:, 0],\n",
    "    \"end_y\": pred[:, 1],\n",
    "})\n",
    "\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSaved submission.csv\")\n",
    "print(sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (6) meta 저장\n",
    "meta_path = os.path.join(MODEL_DIR, \"model_meta.json\")\n",
    "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        **final,\n",
    "        \"oof_base_or_branch_score\": float(base_ref),\n",
    "        \"oof_after_postprocess_score\": float(best_pp[\"score\"]) if use_postprocess else float(base_ref),\n",
    "        \"feature_cols\": best_fcols,\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved meta:\", meta_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
