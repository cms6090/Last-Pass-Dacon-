{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a8d2ecb",
   "metadata": {},
   "source": [
    "### ============================================================\n",
    "### AutoGluon Tabular 기반 end_x/end_y 회귀 파이프라인 (수정본)\n",
    "1) GroupKFold(game_id) OOF 평가(유클리드)\n",
    "2) object 결측치 처리 일관화 (학습/OOF/importance/최종/테스트)\n",
    "3) 임시 predictor 폴더 try/finally 정리\n",
    "4) (누수 완화) feature importance pruning을 CV-train에서만 누적\n",
    "5) branching OOF: valid subset만 예측 + 안전장치\n",
    "6) OOF 결과 디스크 캐시 (후보 비교 반복 학습 낭비 제거)\n",
    "7) k_prev 탐색은 cheap preset(빠르게), 본 탐색은 good, 최종은 best\n",
    "### ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b2b706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy pandas tqdm catboost scikit-learn\n",
    "# !pip install -U autogluon\n",
    "# !pip install -U torch torchvision --index-url https://download.pytorch.org/whl/cu130\n",
    "\n",
    "import os, json, shutil, hashlib, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from autogluon.tabular import TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6904440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_GPUS = 1\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# GPU 확인 (환경 점검용)\n",
    "# -------------------------\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "def get_num_gpus():\n",
    "    \"\"\"\n",
    "    GPU 자동 감지:\n",
    "      - torch import 가능 + cuda available -> 1\n",
    "      - 아니면 0\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        return 1 if torch.cuda.is_available() else 0\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "NUM_GPUS = get_num_gpus()\n",
    "print(\"NUM_GPUS =\", NUM_GPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45a51e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 경로 설정\n",
    "# -------------------------\n",
    "ART_DIR       = \"artifacts\"    # preprocess에서 만든 parquet 폴더\n",
    "MODEL_DIR     = \"models_ag\"    # 최종 predictor 저장 폴더\n",
    "TMP_DIR       = \"ag_tmp\"       # fold별 임시 predictor 저장 폴더\n",
    "OOF_CACHE_DIR = \"oof_cache\"    # OOF 캐시 폴더\n",
    "PRUNE_CACHE_DIR = \"prune_cache\"\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(TMP_DIR, exist_ok=True)\n",
    "os.makedirs(OOF_CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(PRUNE_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# 경기장 상수(좌표 클리핑/후처리에 사용)\n",
    "# -------------------------\n",
    "PITCH_X, PITCH_Y = 105.0, 68.0\n",
    "GOAL_Y = 34.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac3f481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Global option: parallel/sequential\n",
    "#   - \"parallel_local\": AG 내부 bagging/stacking fold 전략을 parallel로\n",
    "#   - \"sequential_local\": 안정성 우선\n",
    "#   - None: AG 기본값\n",
    "# -------------------------\n",
    "FOLD_FITTING_STRATEGY = \"sequential_local\"  # 필요시 \"parallel_local\"로 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0af46b2",
   "metadata": {},
   "source": [
    "### Utils: 평가/클리핑/데이터 로딩/전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46ee79f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_mean_distance(y_true_xy: np.ndarray, y_pred_xy: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    (end_x, end_y) 2차원 예측의 평균 유클리드 거리.\n",
    "    - 최종 평가 지표(OOF 비교/모델 선택/후처리 튜닝)에 사용\n",
    "    \"\"\"\n",
    "    diff = y_true_xy - y_pred_xy\n",
    "    return float(np.sqrt(diff[:, 0]**2 + diff[:, 1]**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "215c0664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_xy(px, py):\n",
    "    \"\"\"\n",
    "    예측 좌표를 경기장 범위로 강제 클리핑\n",
    "    - x: [0, 105], y: [0, 68]\n",
    "    - OOF/테스트 예측 모두에 적용해 비현실적인 값 방지\n",
    "    \"\"\"\n",
    "    return np.clip(px, 0, PITCH_X), np.clip(py, 0, PITCH_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1035f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_object_missing(df: pd.DataFrame, cols=None, fill_value=\"MISSING\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    object 컬럼의 NaN을 일관되게 채우기\n",
    "\n",
    "    - 원 코드에선 X_all만 채우고 실제 학습 df(tr/va/train_full)는 안 채워져서 불안정 가능\n",
    "    - OOF/importance/최종학습/테스트 모두 여기로 통일\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    use_cols = cols if cols is not None else out.columns\n",
    "    for c in use_cols:\n",
    "        if c in out.columns and out[c].dtype == \"object\":\n",
    "            out[c] = out[c].fillna(fill_value)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0653ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pack(k_prev: int):\n",
    "    \"\"\"\n",
    "    preprocess에서 저장한 k_prev 버전의 train/test feature + label을 로드\n",
    "    - 입력: artifacts/features_train_k{k}.parquet, labels_train_k{k}.parquet, features_test_k{k}.parquet\n",
    "    - 출력: X_train(피처), y_train(라벨), X_test(피처)\n",
    "    \"\"\"\n",
    "    X_train = pd.read_parquet(os.path.join(ART_DIR, f\"features_train_k{k_prev}.parquet\"))\n",
    "    y_train = pd.read_parquet(os.path.join(ART_DIR, f\"labels_train_k{k_prev}.parquet\"))\n",
    "    X_test  = pd.read_parquet(os.path.join(ART_DIR, f\"features_test_k{k_prev}.parquet\"))\n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91658136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X_train, y_train):\n",
    "    \"\"\"\n",
    "    학습용 테이블(data)을 만들고, GroupKFold를 위한 groups(game_id)와 feature 컬럼 리스트를 구성.\n",
    "    AutoGluon은 fit(train_data=...)에 label 컬럼이 포함된 df를 넣어야 하므로 merge된 data를 중심으로 사용.\n",
    "\n",
    "    - data: (피처 + end_x/end_y 라벨 포함) merged df\n",
    "    - groups: GroupKFold 분리용 game_id\n",
    "    - feat_cols: 모델 입력으로 사용할 피처 컬럼 목록\n",
    "    \"\"\"\n",
    "    # 1) episode 단위로 피처와 라벨을 합침\n",
    "    data = X_train.merge(y_train, on=\"game_episode\", how=\"inner\")\n",
    "\n",
    "    # 2) group split 단위는 game_id (같은 game의 episode들이 fold를 넘나들지 않게)\n",
    "    groups = data[\"game_id\"].values\n",
    "\n",
    "    # 3) feature 컬럼은 label/키 제외\n",
    "    drop_cols = {\"game_episode\", \"end_x\", \"end_y\"}\n",
    "    feat_cols = [c for c in data.columns if c not in drop_cols]\n",
    "\n",
    "    # 4) 참고용(평가용 라벨 배열)\n",
    "    yx = data[\"end_x\"].values\n",
    "    yy = data[\"end_y\"].values\n",
    "\n",
    "    return data, yx, yy, groups, feat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3117573f",
   "metadata": {},
   "source": [
    "### 0-1) OOF 캐시 유틸 (후보 비교 반복 학습 제거)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4764ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_oof_key(k_prev_fixed, feat_cols, x_metric, y_metric, n_splits, tag=\"base\"):\n",
    "    \"\"\"\n",
    "    OOF 캐시 키 생성\n",
    "    - feature 리스트가 길어서 md5 hash로 축약\n",
    "    - tag로 목적(예: 'cand', 'ksearch') 구분 가능\n",
    "    \"\"\"\n",
    "    h = hashlib.md5((\"|\".join(feat_cols)).encode(\"utf-8\")).hexdigest()[:10]\n",
    "    return f\"{tag}_k{k_prev_fixed}_f{h}_{x_metric}_{y_metric}_cv{n_splits}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e04f17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_oof_cache(key):\n",
    "    \"\"\"캐시 존재 시 (score, oof_pred) 반환\"\"\"\n",
    "    path = os.path.join(OOF_CACHE_DIR, f\"{key}.pkl\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6599ed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_oof_cache(key, obj):\n",
    "    \"\"\"(score, oof_pred) 저장\"\"\"\n",
    "    path = os.path.join(OOF_CACHE_DIR, f\"{key}.pkl\")\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b04f9c0",
   "metadata": {},
   "source": [
    "### 1) AutoGluon helper: fold 1개 학습 / OOF 생성 / OOF 점수 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1b3e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ag_fit_one_fold(\n",
    "    train_df: pd.DataFrame,\n",
    "    label: str,\n",
    "    eval_metric=\"rmse\",\n",
    "    presets=\"good_quality\",\n",
    "    time_limit=None,\n",
    "    path=None,\n",
    "    num_gpus=NUM_GPUS,\n",
    "    fold_fitting_strategy=None,   # \"parallel_local\" / \"sequential_local\" / None\n",
    "    extra_fit_kwargs=None,        # dict 형태로 추가 fit kwargs 주입 가능\n",
    "):\n",
    "    \"\"\"\n",
    "    (단일 fold) AutoGluon TabularPredictor 학습\n",
    "\n",
    "    - train_df: feature + label이 포함된 DF\n",
    "    - label: 학습할 타깃 컬럼명 (\"end_x\" 또는 \"end_y\")\n",
    "    - eval_metric: AutoGluon이 내부 모델 선택/평가에 사용할 metric (rmse/mae 등)\n",
    "    - presets: 학습 품질/시간 트레이드오프 템플릿\n",
    "    - time_limit: 이 fold 학습에 허용할 총 시간(초). None이면 제한 없음\n",
    "    - path: predictor 저장 경로(폴더). 이미 있으면 삭제 후 재생성\n",
    "    \"\"\"\n",
    "    if path is not None and os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "\n",
    "    fit_kwargs = dict(\n",
    "        train_data=train_df,\n",
    "        presets=presets,\n",
    "        time_limit=time_limit,\n",
    "        ag_args_fit={\"num_gpus\": int(num_gpus)},\n",
    "    )\n",
    "\n",
    "    if fold_fitting_strategy is not None:\n",
    "        fit_kwargs[\"ag_args_ensemble\"] = {\"fold_fitting_strategy\": fold_fitting_strategy}\n",
    "\n",
    "    if extra_fit_kwargs:\n",
    "        fit_kwargs.update(extra_fit_kwargs)\n",
    "\n",
    "    predictor = TabularPredictor(\n",
    "        label=label,\n",
    "        problem_type=\"regression\",\n",
    "        eval_metric=eval_metric,\n",
    "        path=path,\n",
    "    ).fit(**fit_kwargs)\n",
    "\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ac3dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ag_oof_predict(\n",
    "    df_all: pd.DataFrame,\n",
    "    label: str,\n",
    "    feat_cols: list,\n",
    "    groups: np.ndarray,\n",
    "    n_splits=5,\n",
    "    eval_metric=\"rmse\",\n",
    "    presets=\"good_quality\",\n",
    "    time_limit=None,\n",
    "    save_root=TMP_DIR,\n",
    "    num_gpus=NUM_GPUS,\n",
    "    fold_fitting_strategy=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    GroupKFold 기반 OOF 예측 생성.\n",
    "    - fold마다 predictor를 학습하고(valid fold에는 절대 학습 데이터가 들어가지 않음)\n",
    "      valid fold를 예측한 값을 oof 배열에 채움.\n",
    "\n",
    "    반환:\n",
    "    - oof: 길이 N(샘플 수)짜리 예측값 배열\n",
    "    \"\"\"\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    oof = np.zeros(len(df_all), dtype=float)\n",
    "\n",
    "    # gkf.split의 y는 사실상 형식상 필요(여기서는 df_all[label]을 넣음)\n",
    "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(df_all[feat_cols], df_all[label], groups=groups)):\n",
    "        # 학습 df: feature + label 포함\n",
    "        tr = df_all.iloc[tr_idx][feat_cols + [label]].copy()\n",
    "        va = df_all.iloc[va_idx][feat_cols].copy()\n",
    "        \n",
    "        # 검증 df: feature만 (예측용), object 결측 일관 처리\n",
    "        tr = fill_object_missing(tr, cols=feat_cols)\n",
    "        va = fill_object_missing(va, cols=feat_cols)\n",
    "\n",
    "        # fold별 predictor 저장 폴더\n",
    "        path = os.path.join(save_root, f\"{label}_fold{fold}\")\n",
    "        \n",
    "        try:\n",
    "            predictor = ag_fit_one_fold(\n",
    "                train_df=tr,\n",
    "                label=label,\n",
    "                eval_metric=eval_metric,\n",
    "                presets=presets,\n",
    "                time_limit=time_limit,\n",
    "                path=path,\n",
    "                num_gpus=num_gpus,\n",
    "                fold_fitting_strategy=fold_fitting_strategy,\n",
    "            )\n",
    "            \n",
    "            oof[va_idx] = predictor.predict(va).values\n",
    "        finally:\n",
    "            # ✅ 학습 중 에러가 나도 폴더는 정리\n",
    "            if os.path.exists(path):\n",
    "                shutil.rmtree(path)\n",
    "\n",
    "    return oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc0a5825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ag_cv_score_xy(\n",
    "    data_merged: pd.DataFrame,\n",
    "    feat_cols: list,\n",
    "    groups: np.ndarray,\n",
    "    x_metric=\"rmse\",\n",
    "    y_metric=\"rmse\",\n",
    "    n_splits=5,\n",
    "    presets=\"good_quality\",\n",
    "    time_limit=None,\n",
    "    num_gpus=NUM_GPUS,\n",
    "    fold_fitting_strategy=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    end_x/end_y 각각 OOF를 만든 뒤, (x,y) 유클리드 평균 거리로 점수 계산.\n",
    "\n",
    "    반환:\n",
    "    - score: mean euclidean\n",
    "    - oof_xy: shape (N,2) -> [:,0]=px_oof, [:,1]=py_oof\n",
    "    \"\"\"\n",
    "    # end_x OOF\n",
    "    px = ag_oof_predict(\n",
    "        df_all=data_merged, label=\"end_x\", feat_cols=feat_cols, groups=groups,\n",
    "        n_splits=n_splits, eval_metric=x_metric, presets=presets,\n",
    "        time_limit=time_limit, save_root=TMP_DIR, num_gpus=num_gpus,\n",
    "        fold_fitting_strategy=fold_fitting_strategy,\n",
    "    )\n",
    "\n",
    "    # end_y OOF\n",
    "    py = ag_oof_predict(\n",
    "        df_all=data_merged, label=\"end_y\", feat_cols=feat_cols, groups=groups,\n",
    "        n_splits=n_splits, eval_metric=y_metric, presets=presets,\n",
    "        time_limit=time_limit, save_root=TMP_DIR, num_gpus=num_gpus,\n",
    "        fold_fitting_strategy=fold_fitting_strategy,\n",
    "    )\n",
    "\n",
    "    # 경기장 밖으로 나간 예측은 클리핑 후 평가\n",
    "    px, py = clip_xy(px, py)\n",
    "\n",
    "    # 정답\n",
    "    y_true = data_merged[[\"end_x\", \"end_y\"]].values\n",
    "\n",
    "    # 최종 점수(평가 기준은 항상 Euclidean)\n",
    "    score = euclidean_mean_distance(y_true, np.column_stack([px, py]))\n",
    "\n",
    "    return score, np.column_stack([px, py])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ac956d",
   "metadata": {},
   "source": [
    "### 2) pruning: feature importance top-N : 누수 완화 pruning: CV-train에서만 importance 누적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e848ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prune_key(k_prev_fixed, feat_cols, n_splits, top_n, presets, fold_strategy,\n",
    "                   eval_metric=\"rmse\", time_limit=None):\n",
    "    h = hashlib.md5((\"|\".join(feat_cols)).encode(\"utf-8\")).hexdigest()[:10]\n",
    "    fs = str(fold_strategy) if fold_strategy is not None else \"None\"\n",
    "    tl = \"None\" if time_limit is None else str(time_limit)\n",
    "    return f\"prune_k{k_prev_fixed}_f{h}_cv{n_splits}_top{top_n}_{eval_metric}_{presets}_{fs}_t{tl}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba1dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ag_prune_features_cv(\n",
    "    data_merged: pd.DataFrame,\n",
    "    feat_cols: list,\n",
    "    groups: np.ndarray,\n",
    "    n_splits=5,\n",
    "    top_n=200,\n",
    "    eval_metric=\"rmse\",\n",
    "    presets=\"good_quality\",\n",
    "    time_limit=None,\n",
    "    num_gpus=NUM_GPUS,\n",
    "    save_root=TMP_DIR,\n",
    "    fold_fitting_strategy=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    ✅ 누수 완화 pruning:\n",
    "    - 각 fold에서 \"train subset\"으로만 end_x/end_y importance 계산\n",
    "    - fold별 importance 평균을 내서 top_n 피처 선택\n",
    "    \"\"\"\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    imp_sum = pd.Series(0.0, index=feat_cols)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(data_merged[feat_cols], data_merged[\"end_x\"], groups=groups)):\n",
    "        tr_all = data_merged.iloc[tr_idx].copy()\n",
    "\n",
    "        # object 결측 처리\n",
    "        tr_all = fill_object_missing(tr_all, cols=feat_cols)\n",
    "\n",
    "        # fold별 임시 path\n",
    "        path_x = os.path.join(save_root, f\"imp_cv_endx_fold{fold}\")\n",
    "        path_y = os.path.join(save_root, f\"imp_cv_endy_fold{fold}\")\n",
    "\n",
    "        try:\n",
    "            # end_x importance\n",
    "            pred_x = ag_fit_one_fold(\n",
    "                train_df=tr_all[feat_cols + [\"end_x\"]],\n",
    "                label=\"end_x\",\n",
    "                eval_metric=eval_metric,\n",
    "                presets=presets,\n",
    "                time_limit=time_limit,\n",
    "                path=path_x,\n",
    "                num_gpus=num_gpus,\n",
    "                fold_fitting_strategy=fold_fitting_strategy,\n",
    "                extra_fit_kwargs={\n",
    "                    \"dynamic_stacking\": False,\n",
    "                    \"auto_stack\": False,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            imp_x = pred_x.feature_importance(tr_all[feat_cols + [\"end_x\"]], silent=True)[\"importance\"]\n",
    "\n",
    "\n",
    "            # end_y importance\n",
    "            pred_y = ag_fit_one_fold(\n",
    "                train_df=tr_all[feat_cols + [\"end_y\"]],\n",
    "                label=\"end_y\",\n",
    "                eval_metric=eval_metric,\n",
    "                presets=presets,\n",
    "                time_limit=time_limit,\n",
    "                path=path_y,\n",
    "                num_gpus=num_gpus,\n",
    "                fold_fitting_strategy=fold_fitting_strategy,\n",
    "                extra_fit_kwargs={\n",
    "                    \"dynamic_stacking\": False,\n",
    "                    \"auto_stack\": False,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            imp_y = pred_y.feature_importance(tr_all[feat_cols + [\"end_y\"]], silent=True)[\"importance\"]\n",
    "\n",
    "            # fold importance 평균\n",
    "            imp_fold = (imp_x.reindex(feat_cols).fillna(0) + imp_y.reindex(feat_cols).fillna(0)) / 2.0\n",
    "            imp_sum = imp_sum.add(imp_fold, fill_value=0.0)\n",
    "\n",
    "        finally:\n",
    "            for p in [path_x, path_y]:\n",
    "                if os.path.exists(p):\n",
    "                    shutil.rmtree(p)\n",
    "\n",
    "    imp_avg = (imp_sum / n_splits).sort_values(ascending=False)\n",
    "    top_features = imp_avg.head(min(top_n, len(imp_avg))).index.tolist()\n",
    "    return top_features, imp_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199bddd9",
   "metadata": {},
   "source": [
    "### 3) branching CV: result_name으로 성공/실패 분기 학습 + 라우팅 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d29f5aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ag_cv_score_xy_branch(\n",
    "    data_merged: pd.DataFrame,\n",
    "    feat_cols: list,\n",
    "    groups: np.ndarray,\n",
    "    x_metric=\"rmse\",\n",
    "    y_metric=\"rmse\",\n",
    "    result_col=\"result_name\",\n",
    "    n_splits=5,\n",
    "    presets=\"good_quality\",\n",
    "    time_limit=None,\n",
    "    min_side=50,\n",
    "    num_gpus=NUM_GPUS,\n",
    "    fold_fitting_strategy=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    result_name 기준 분기 학습 OOF:\n",
    "    - train fold에서 Successful/Unsuccessful 별도 모델 학습\n",
    "    - valid fold는 result_name으로 라우팅\n",
    "    \n",
    "    - ✅ valid subset만 각각 예측(효율)\n",
    "    - ✅ 한쪽 표본이 너무 적으면 전체 모델로 fallback\n",
    "    \"\"\"\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    oof = np.zeros((len(data_merged), 2), dtype=float)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(data_merged[feat_cols], data_merged[\"end_x\"], groups=groups)):\n",
    "        tr_all = data_merged.iloc[tr_idx].copy()\n",
    "        va_all = data_merged.iloc[va_idx].copy()\n",
    "\n",
    "        # 전처리(학습/예측에 쓰일 feature)\n",
    "        tr_all = fill_object_missing(tr_all, cols=feat_cols)\n",
    "        va_feat = fill_object_missing(va_all[feat_cols].copy(), cols=feat_cols)\n",
    "\n",
    "        # train에서 성공/실패 분리\n",
    "        tr_s = tr_all[result_col].astype(str).eq(\"Successful\")\n",
    "        tr_u = ~tr_s\n",
    "\n",
    "        # fallback: 표본이 너무 적으면 전체 모델\n",
    "        if tr_s.sum() < min_side or tr_u.sum() < min_side:\n",
    "            # fallback: 전체 모델\n",
    "            path_x = os.path.join(TMP_DIR, f\"branch_all_endx_fold{fold}\")\n",
    "            path_y = os.path.join(TMP_DIR, f\"branch_all_endy_fold{fold}\")\n",
    "            try:\n",
    "                px_model = ag_fit_one_fold(\n",
    "                    train_df=tr_all[feat_cols + [\"end_x\"]],\n",
    "                    label=\"end_x\",\n",
    "                    eval_metric=x_metric,\n",
    "                    presets=presets,\n",
    "                    time_limit=time_limit,\n",
    "                    path=path_x,\n",
    "                    num_gpus=num_gpus,\n",
    "                    fold_fitting_strategy=fold_fitting_strategy,\n",
    "                )\n",
    "\n",
    "                py_model = ag_fit_one_fold(\n",
    "                    train_df=tr_all[feat_cols + [\"end_y\"]],\n",
    "                    label=\"end_y\",\n",
    "                    eval_metric=y_metric,\n",
    "                    presets=presets,\n",
    "                    time_limit=time_limit,\n",
    "                    path=path_y,\n",
    "                    num_gpus=num_gpus,\n",
    "                    fold_fitting_strategy=fold_fitting_strategy,\n",
    "                )\n",
    "                px = px_model.predict(va_feat).values\n",
    "                py = py_model.predict(va_feat).values\n",
    "            finally:\n",
    "                for p in [path_x, path_y]:\n",
    "                    if os.path.exists(p):\n",
    "                        shutil.rmtree(p)\n",
    "\n",
    "        # -------- 분기 학습: 성공/실패 각각 별도 predictor 학습 --------\n",
    "        else:\n",
    "            # 성공/실패 모델 각각 학습\n",
    "            path_sx = os.path.join(TMP_DIR, f\"branch_s_endx_fold{fold}\")\n",
    "            path_sy = os.path.join(TMP_DIR, f\"branch_s_endy_fold{fold}\")\n",
    "            path_ux = os.path.join(TMP_DIR, f\"branch_u_endx_fold{fold}\")\n",
    "            path_uy = os.path.join(TMP_DIR, f\"branch_u_endy_fold{fold}\")\n",
    "\n",
    "            try:\n",
    "                px_s = ag_fit_one_fold(\n",
    "                    train_df=tr_all.loc[tr_s, feat_cols + [\"end_x\"]],\n",
    "                    label=\"end_x\",\n",
    "                    eval_metric=x_metric,\n",
    "                    presets=presets,\n",
    "                    time_limit=time_limit,\n",
    "                    path=path_sx,\n",
    "                    num_gpus=num_gpus,\n",
    "                    fold_fitting_strategy=fold_fitting_strategy,\n",
    "                )\n",
    "                py_s = ag_fit_one_fold(\n",
    "                    train_df=tr_all.loc[tr_s, feat_cols + [\"end_y\"]],\n",
    "                    label=\"end_y\",\n",
    "                    eval_metric=y_metric,\n",
    "                    presets=presets,\n",
    "                    time_limit=time_limit,\n",
    "                    path=path_sy,\n",
    "                    num_gpus=num_gpus,\n",
    "                    fold_fitting_strategy=fold_fitting_strategy,\n",
    "                )\n",
    "                px_u = ag_fit_one_fold(\n",
    "                    train_df=tr_all.loc[tr_u, feat_cols + [\"end_x\"]],\n",
    "                    label=\"end_x\",\n",
    "                    eval_metric=x_metric,\n",
    "                    presets=presets,\n",
    "                    time_limit=time_limit,\n",
    "                    path=path_ux,\n",
    "                    num_gpus=num_gpus,\n",
    "                    fold_fitting_strategy=fold_fitting_strategy,\n",
    "                )\n",
    "                py_u = ag_fit_one_fold(\n",
    "                    train_df=tr_all.loc[tr_u, feat_cols + [\"end_y\"]],\n",
    "                    label=\"end_y\",\n",
    "                    eval_metric=y_metric,\n",
    "                    presets=presets,\n",
    "                    time_limit=time_limit,\n",
    "                    path=path_uy,\n",
    "                    num_gpus=num_gpus,\n",
    "                    fold_fitting_strategy=fold_fitting_strategy,\n",
    "                )\n",
    "\n",
    "                # valid 라우팅 마스크\n",
    "                va_s_mask = va_all[result_col].astype(str).eq(\"Successful\").values\n",
    "                px = np.empty(len(va_all), dtype=float)\n",
    "                py = np.empty(len(va_all), dtype=float)\n",
    "\n",
    "                # ✅ valid subset만 예측\n",
    "                idx_s = np.where(va_s_mask)[0]\n",
    "                idx_u = np.where(~va_s_mask)[0]\n",
    "                if len(idx_s):\n",
    "                    px[idx_s] = px_s.predict(va_feat.iloc[idx_s]).values\n",
    "                    py[idx_s] = py_s.predict(va_feat.iloc[idx_s]).values\n",
    "                if len(idx_u):\n",
    "                    px[idx_u] = px_u.predict(va_feat.iloc[idx_u]).values\n",
    "                    py[idx_u] = py_u.predict(va_feat.iloc[idx_u]).values\n",
    "\n",
    "            finally:\n",
    "                for p in [path_sx, path_sy, path_ux, path_uy]:\n",
    "                    if os.path.exists(p):\n",
    "                        shutil.rmtree(p)\n",
    "\n",
    "        px, py = clip_xy(px, py)\n",
    "        oof[va_idx, 0] = px\n",
    "        oof[va_idx, 1] = py\n",
    "\n",
    "    y_true = data_merged[[\"end_x\", \"end_y\"]].values\n",
    "    score = euclidean_mean_distance(y_true, oof)\n",
    "    return score, oof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0143b39b",
   "metadata": {},
   "source": [
    "### 4) 후처리(postprocess) 함수: 예측점을 start 기준으로 보정(shrink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51afa048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_postprocess(df_feat: pd.DataFrame, pred_xy: np.ndarray,\n",
    "                      forward_scale: float, lateral_shrink: float):\n",
    "    \"\"\"\n",
    "    후처리 아이디어:\n",
    "    - pred_x는 start_x 기준으로 전진/후진 변위를 forward_scale만큼 스케일\n",
    "    - pred_y는 중앙선(GOAL_Y=34) 기준으로 lateral_shrink만큼 중앙으로 당김\n",
    "\n",
    "    주의: df_feat에 start_x가 반드시 있어야 함.\n",
    "    \"\"\"\n",
    "    start_x = df_feat[\"start_x\"].values\n",
    "\n",
    "    # x: start 기준 변위 스케일링\n",
    "    pred_x = start_x + (pred_xy[:, 0] - start_x) * forward_scale\n",
    "\n",
    "    # y: 중앙선 기준 shrink\n",
    "    pred_y = GOAL_Y + (pred_xy[:, 1] - GOAL_Y) * lateral_shrink\n",
    "\n",
    "    # 보정 후에도 경기장 범위로 클리핑\n",
    "    pred_x, pred_y = clip_xy(pred_x, pred_y)\n",
    "    return np.column_stack([pred_x, pred_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058df2cf",
   "metadata": {},
   "source": [
    "### 5) 메인 실행(STEP 1 ~ 6)\n",
    "1) best_k_prev를 뽑고, 이후에는 best_k_prev만 사용\n",
    "2) 캐시 키 생성에도 best_k_prev만 사용 (k_prev 잔존 변수 사용 금지)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7962f32c",
   "metadata": {},
   "source": [
    "#### 하이퍼/프리셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7de637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_LIST   = [3, 5, 7, 10]\n",
    "N_SPLITS = 5\n",
    "\n",
    "TIME_LIMIT_PER_FOLD = None\n",
    "\n",
    "SEARCH_PRESETS_K = \"medium_quality\"  # k_prev 탐색(cheap)\n",
    "SEARCH_PRESETS   = \"good_quality\"    # 후보 비교/프루닝\n",
    "FINAL_PRESETS    = \"best_quality\"    # 최종 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb86c714",
   "metadata": {},
   "source": [
    "#### STEP 1: k_prev(k개의 과거 정보 이용) 선택 (cheap preset + OOF 캐시)\n",
    "**Pass가 시작되기 직전 몇 개(k개)의 과거 이벤트 정보를 피처로 사용할지(k_prev) 결정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7911cd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== STEP1: Search k_prev=3 (cheap preset) ====\n",
      "[CACHE HIT] ksearch_k3_f7086118df8_rmse_rmse_cv5 -> 14.169963\n",
      "\n",
      "==== STEP1: Search k_prev=5 (cheap preset) ====\n",
      "[CACHE HIT] ksearch_k5_fab10542a08_rmse_rmse_cv5 -> 14.280500\n",
      "\n",
      "==== STEP1: Search k_prev=7 (cheap preset) ====\n",
      "[CACHE HIT] ksearch_k7_ffcddc01270_rmse_rmse_cv5 -> 14.351664\n",
      "\n",
      "==== STEP1: Search k_prev=10 (cheap preset) ====\n",
      "[CACHE HIT] ksearch_k10_feed60920f8_rmse_rmse_cv5 -> 14.394723\n"
     ]
    }
   ],
   "source": [
    "best_k = {\"k_prev\": None, \"oof\": float(\"inf\"), \"oof_pred\": None}\n",
    "\n",
    "# k_prev별로 OOF score를 계산 -> 가장 작은 score를 best로 선택\n",
    "for k in K_LIST:\n",
    "    print(f\"\\n==== STEP1: Search k_prev={k} (cheap preset) ====\")\n",
    "    \"\"\"\n",
    "    1. 각 k_prev에 대해 5-fold OOF로 end_x/end_y를 예측하고,\n",
    "    2. fold마다 AutoGluon이 선택한 모델들의 예측을 합쳐 계산한\n",
    "    3. 전체 OOF 유클리드 평균 오차를 기준으로\n",
    "    4. k_prev 중 가장 좋은 값을 선택\n",
    "\n",
    "    각 fold(5) 당 x, y를 학습 -> 총 5 * 2회\n",
    "    이거를 각 k_prev[3, 5, 7, 10]마다 실행 -> 총 4 * (5 * 2)회\n",
    "    \"\"\"\n",
    "    # k_prev별 데이터 로드\n",
    "    X_train, y_train, _ = load_pack(k)\n",
    "\n",
    "    # merged data + 그룹 + feature 컬럼\n",
    "    data_k, _, _, groups_k, feat_cols_k = prepare_data(X_train, y_train)\n",
    "\n",
    "    # ✅ k 탐색도 캐시 사용(자주 재실행하니까)\n",
    "    key = make_oof_key(k, feat_cols_k, \"rmse\", \"rmse\", N_SPLITS, tag=\"ksearch\")\n",
    "    cached = load_oof_cache(key)\n",
    "\n",
    "    if cached is not None:\n",
    "        score, oof_pred = cached\n",
    "        print(f\"[CACHE HIT] {key} -> {score:.6f}\")\n",
    "        \n",
    "    else:\n",
    "        score, oof_pred = ag_cv_score_xy(\n",
    "            data_merged=data_k,\n",
    "            feat_cols=feat_cols_k,\n",
    "            groups=groups_k,\n",
    "            x_metric=\"rmse\",\n",
    "            y_metric=\"rmse\",\n",
    "            n_splits=N_SPLITS,\n",
    "            presets=SEARCH_PRESETS_K,\n",
    "            time_limit=TIME_LIMIT_PER_FOLD,\n",
    "            num_gpus=NUM_GPUS,\n",
    "            fold_fitting_strategy=FOLD_FITTING_STRATEGY,\n",
    "        )\n",
    "        save_oof_cache(key, (score, oof_pred))\n",
    "        print(f\"[TRAINED] {key} -> {score:.6f}\")\n",
    "\n",
    "    if score < best_k[\"oof\"]:\n",
    "        best_k.update({\"k_prev\": k, \"oof\": score, \"oof_pred\": oof_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc36835f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEST k_prev: 3 baseline OOF: 14.169962815260774\n"
     ]
    }
   ],
   "source": [
    "best_k_prev = int(best_k[\"k_prev\"])\n",
    "print(\"\\nBEST k_prev:\", best_k_prev, \"baseline OOF:\", best_k[\"oof\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3798356",
   "metadata": {},
   "source": [
    "#### STEP 2: 선택된 k_prev에서 pruning (importance top TOP_N) : 피처 선택\n",
    "**STEP 1에서 선택된 피처 셋에서 모델에 불필요하거나 노이즈가 많은 피처를 제거**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7f462f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 20:57:53,634\tINFO worker.py:1852 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c68db8c322449eb2c6f0c15a8f966c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n",
       "  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n",
       "    <g clip-path=\"url(#clip0_4338_178347)\">\n",
       "        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n",
       "        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n",
       "    </g>\n",
       "    <defs>\n",
       "        <clipPath id=\"clip0_4338_178347\">\n",
       "            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n",
       "        </clipPath>\n",
       "    </defs>\n",
       "  </svg>\n",
       "</div>\n",
       "\n",
       "        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.10.11</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>2.44.1</b></td>\n",
       "    </tr>\n",
       "    \n",
       "</table>\n",
       "\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.10.11', ray_version='2.44.1', ray_commit='daca7b2b1a950dc7f731e34e74c76ae383794ffe')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "ray.shutdown()\n",
    "ray.init(num_gpus=NUM_GPUS, include_dashboard=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "943055cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'object_store_memory': 5784021811.0, 'GPU': 1.0, 'memory': 13496050893.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'node:127.0.0.1': 1.0, 'CPU': 12.0}\n"
     ]
    }
   ],
   "source": [
    "print(ray.cluster_resources())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a58558f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          12\n",
      "Memory Avail:       17.27 GB / 31.59 GB (54.7%)\n",
      "Disk Space Avail:   25.90 GB / 100.00 GB (25.9%)\n",
      "===================================================\n",
      "Presets specified: ['good_quality']\n",
      "Using hyperparameters preset: hyperparameters='light'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
      "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tWarning: No time limit provided for DyStack. This could take awhile.\n",
      "\t\tContext path: \"d:\\공모전\\스포츠\\ag_tmp\\imp_cv_endx_fold0\\ds_sub_fit\\sub_fit_ho\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== STEP2: Pruning via CV-train importance ====\n",
      "STEP2 using k_prev = 3 TOP_N = 53\n",
      "[PRUNE CACHE MISS] run pruning: prune_k3_f7086118df8_cv5_top53_rmse_good_quality_sequential_local_tNone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=12004)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Beginning AutoGluon training ...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\imp_cv_endx_fold0\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Train Data Rows:    10954\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Train Data Columns: 53\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Label Column:       end_x\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Problem Type:       regression\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tAvailable Memory:                    17941.11 MB\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tTrain Data (Original)  Memory Usage: 8.50 MB (0.0% of available memory)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\t('bool', [])   :  1 | ['is_home']\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\t('float', [])  : 40 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\t('object', []) :  7 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\t('category', [])  :  6 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\t('float', [])     : 40 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t0.1s = Fit runtime\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t53 features in original data used to generate 53 features in processed data.\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tTrain Data (Processed) Memory Usage: 3.77 MB (0.0% of available memory)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Data preprocessing and feature engineering runtime = 0.09s ...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t'NN_TORCH': [{}],\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t'CAT': [{}],\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t'XGB': [{}],\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t'FASTAI': [{}],\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tWarning: Exception caused LightGBMXT_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\tSpecified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2171, in _train_and_save\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1046, in fit\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     kwargs = self._preprocess_fit_args(**kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 690, in _preprocess_fit_args\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     kwargs = self._preprocess_fit_resources(**kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 949, in _preprocess_fit_resources\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     return self._calculate_total_resources(silent=silent, total_resources=total_resources, parallel_hpo=parallel_hpo, **kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 821, in _calculate_total_resources\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     user_specified_lower_level_num_gpus = self._process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble(\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 766, in _process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     assert user_specified_model_level_resource <= system_resource, f\"Specified {resource_type} per model base is more than the total: {system_resource}\"\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m AssertionError: Specified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Fitting model: LightGBM_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tWarning: Exception caused LightGBM_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\tSpecified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2171, in _train_and_save\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1046, in fit\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     kwargs = self._preprocess_fit_args(**kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 690, in _preprocess_fit_args\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     kwargs = self._preprocess_fit_resources(**kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 949, in _preprocess_fit_resources\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     return self._calculate_total_resources(silent=silent, total_resources=total_resources, parallel_hpo=parallel_hpo, **kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 821, in _calculate_total_resources\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     user_specified_lower_level_num_gpus = self._process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble(\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 766, in _process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     assert user_specified_model_level_resource <= system_resource, f\"Specified {resource_type} per model base is more than the total: {system_resource}\"\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m AssertionError: Specified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Fitting model: RandomForestMSE_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tWarning: Exception caused RandomForestMSE_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\tSpecified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2171, in _train_and_save\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1046, in fit\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     kwargs = self._preprocess_fit_args(**kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 690, in _preprocess_fit_args\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     kwargs = self._preprocess_fit_resources(**kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 949, in _preprocess_fit_resources\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     return self._calculate_total_resources(silent=silent, total_resources=total_resources, parallel_hpo=parallel_hpo, **kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 821, in _calculate_total_resources\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     user_specified_lower_level_num_gpus = self._process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble(\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 766, in _process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     assert user_specified_model_level_resource <= system_resource, f\"Specified {resource_type} per model base is more than the total: {system_resource}\"\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m AssertionError: Specified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Fitting model: CatBoost_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tWarning: Exception caused CatBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\tSpecified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2171, in _train_and_save\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1046, in fit\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     kwargs = self._preprocess_fit_args(**kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 690, in _preprocess_fit_args\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     kwargs = self._preprocess_fit_resources(**kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 949, in _preprocess_fit_resources\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     return self._calculate_total_resources(silent=silent, total_resources=total_resources, parallel_hpo=parallel_hpo, **kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 821, in _calculate_total_resources\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     user_specified_lower_level_num_gpus = self._process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble(\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 766, in _process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     assert user_specified_model_level_resource <= system_resource, f\"Specified {resource_type} per model base is more than the total: {system_resource}\"\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m AssertionError: Specified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tWarning: Exception caused ExtraTreesMSE_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\tSpecified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2171, in _train_and_save\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1046, in fit\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     kwargs = self._preprocess_fit_args(**kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 690, in _preprocess_fit_args\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     kwargs = self._preprocess_fit_resources(**kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 949, in _preprocess_fit_resources\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     return self._calculate_total_resources(silent=silent, total_resources=total_resources, parallel_hpo=parallel_hpo, **kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 821, in _calculate_total_resources\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     user_specified_lower_level_num_gpus = self._process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble(\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 766, in _process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     assert user_specified_model_level_resource <= system_resource, f\"Specified {resource_type} per model base is more than the total: {system_resource}\"\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m AssertionError: Specified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\tSpecified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2171, in _train_and_save\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1046, in fit\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     kwargs = self._preprocess_fit_args(**kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 690, in _preprocess_fit_args\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     kwargs = self._preprocess_fit_resources(**kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 949, in _preprocess_fit_resources\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     return self._calculate_total_resources(silent=silent, total_resources=total_resources, parallel_hpo=parallel_hpo, **kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 821, in _calculate_total_resources\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     user_specified_lower_level_num_gpus = self._process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble(\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 766, in _process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     assert user_specified_model_level_resource <= system_resource, f\"Specified {resource_type} per model base is more than the total: {system_resource}\"\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m AssertionError: Specified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Fitting model: XGBoost_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\tSpecified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2171, in _train_and_save\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1046, in fit\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     kwargs = self._preprocess_fit_args(**kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 690, in _preprocess_fit_args\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     kwargs = self._preprocess_fit_resources(**kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 949, in _preprocess_fit_resources\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     return self._calculate_total_resources(silent=silent, total_resources=total_resources, parallel_hpo=parallel_hpo, **kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 821, in _calculate_total_resources\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     user_specified_lower_level_num_gpus = self._process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble(\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 766, in _process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     assert user_specified_model_level_resource <= system_resource, f\"Specified {resource_type} per model base is more than the total: {system_resource}\"\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m AssertionError: Specified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tWarning: Exception caused NeuralNetTorch_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\tSpecified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2171, in _train_and_save\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1046, in fit\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     kwargs = self._preprocess_fit_args(**kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 690, in _preprocess_fit_args\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     kwargs = self._preprocess_fit_resources(**kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 949, in _preprocess_fit_resources\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     return self._calculate_total_resources(silent=silent, total_resources=total_resources, parallel_hpo=parallel_hpo, **kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 821, in _calculate_total_resources\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     user_specified_lower_level_num_gpus = self._process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble(\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 766, in _process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     assert user_specified_model_level_resource <= system_resource, f\"Specified {resource_type} per model base is more than the total: {system_resource}\"\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m AssertionError: Specified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \tWarning: Exception caused LightGBMLarge_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m \t\tSpecified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2171, in _train_and_save\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1046, in fit\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     kwargs = self._preprocess_fit_args(**kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 690, in _preprocess_fit_args\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     kwargs = self._preprocess_fit_resources(**kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 949, in _preprocess_fit_resources\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     return self._calculate_total_resources(silent=silent, total_resources=total_resources, parallel_hpo=parallel_hpo, **kwargs)\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 821, in _calculate_total_resources\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     user_specified_lower_level_num_gpus = self._process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble(\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m   File \"d:\\공모전\\스포츠\\venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 766, in _process_user_provided_resource_requirement_to_calculate_total_resource_when_ensemble\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m     assert user_specified_model_level_resource <= system_resource, f\"Specified {resource_type} per model base is more than the total: {system_resource}\"\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m AssertionError: Specified num_gpus per model base is more than the total: 0\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m No base models to train on, skipping auxiliary stack level 2...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m No base models to train on, skipping stack level 2...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m No base models to train on, skipping auxiliary stack level 3...\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m Warning: AutoGluon did not successfully train any models\n",
      "\u001b[36m(_dystack pid=12004)\u001b[0m AutoGluon training complete, total runtime = 4.0s ... Best model: None\n",
      "Warning: Exception encountered during DyStack sub-fit:\n",
      "\tNo models were trained successfully during fit(). Inspect the log output or increase verbosity to determine why no models were fit. Alternatively, set `raise_on_no_models_fitted` to False during the fit call.\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t6s\t = DyStack   runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"d:\\공모전\\스포츠\\ag_tmp\\imp_cv_endx_fold0\"\n",
      "Train Data Rows:    12324\n",
      "Train Data Columns: 53\n",
      "Label Column:       end_x\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17588.83 MB\n",
      "\tTrain Data (Original)  Memory Usage: 9.56 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   :  1 | ['is_home']\n",
      "\t\t('float', [])  : 40 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])    :  5 | ['game_id', 'period_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('object', []) :  7 | ['result_name', 'prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  6 | ['prev1_type_name', 'prev1_result_name', 'prev2_type_name', 'prev2_result_name', 'prev3_type_name', ...]\n",
      "\t\t('float', [])     : 40 | ['time_seconds', 'start_x', 'start_y', 'start_dist_to_goal', 'start_angle_to_goal', ...]\n",
      "\t\t('int', [])       :  4 | ['game_id', 'team_id', 'player_id', 'ep_len_before']\n",
      "\t\t('int', ['bool']) :  3 | ['period_id', 'is_home', 'result_name']\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 4.25 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=6, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-12.0077\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.69s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=6, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-11.9838\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.98s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ...\n",
      "\t-12.3479\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.07s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=6, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n"
     ]
    }
   ],
   "source": [
    "# 선택된 k_prev pack 로드\n",
    "# ✅ STEP2 시작 (반드시 best_k_prev 사용)\n",
    "X_train, y_train, X_test = load_pack(best_k_prev)\n",
    "data, yx, yy, groups, feat_cols = prepare_data(X_train, y_train)\n",
    "\n",
    "TOP_N = min(200, len(feat_cols))\n",
    "print(\"\\n==== STEP2: Pruning via CV-train importance ====\")\n",
    "print(\"STEP2 using k_prev =\", best_k_prev, \"TOP_N =\", TOP_N)\n",
    "\n",
    "prune_key = make_prune_key(\n",
    "    k_prev_fixed=best_k_prev,\n",
    "    feat_cols=feat_cols,\n",
    "    n_splits=N_SPLITS,\n",
    "    top_n=TOP_N,\n",
    "    presets=SEARCH_PRESETS,\n",
    "    fold_strategy=FOLD_FITTING_STRATEGY,\n",
    "    eval_metric=\"rmse\",\n",
    "    time_limit=TIME_LIMIT_PER_FOLD,\n",
    ")\n",
    "\n",
    "prune_cache_path = os.path.join(PRUNE_CACHE_DIR, f\"{prune_key}.pkl\")\n",
    "\n",
    "if os.path.exists(prune_cache_path):\n",
    "    with open(prune_cache_path, \"rb\") as f:\n",
    "        pruned_features, imp_all = pickle.load(f)\n",
    "    print(f\"[PRUNE CACHE HIT] {prune_key}\")\n",
    "\n",
    "else:\n",
    "    print(f\"[PRUNE CACHE MISS] run pruning: {prune_key}\")\n",
    "    \"\"\"\n",
    "    STEP1에서 선택된 k_prev 데이터에서 5-fold를 돌면서 \n",
    "    각 fold의 train subset으로 end_x/end_y 모델을 각각 학습(총 5 * 2회)하고, \n",
    "    importance도 각각 계산(총 10회)한 뒤, \n",
    "    fold 평균 importance 상위 TOP_N 피처만 남김\n",
    "    \"\"\"\n",
    "    pruned_features, imp_all = ag_prune_features_cv(\n",
    "        data_merged=data,\n",
    "        feat_cols=feat_cols,\n",
    "        groups=groups,\n",
    "        n_splits=N_SPLITS,\n",
    "        top_n=TOP_N,\n",
    "        eval_metric=\"rmse\",\n",
    "        presets=SEARCH_PRESETS,\n",
    "        time_limit=TIME_LIMIT_PER_FOLD,\n",
    "        num_gpus=NUM_GPUS,\n",
    "        save_root=TMP_DIR,\n",
    "        fold_fitting_strategy=FOLD_FITTING_STRATEGY,\n",
    "    )\n",
    "    with open(prune_cache_path, \"wb\") as f:\n",
    "        pickle.dump((pruned_features, imp_all), f)\n",
    "    print(f\"[PRUNE CACHE SAVE] {prune_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3b5b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 20 pruned features:\\n\", imp_all.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd838ca",
   "metadata": {},
   "source": [
    "#### STEP 3: metric(=loss 성격) 분리 비교 (baseline vs pruned) : “모델 후보” 평가\n",
    "**피처 셋과 AutoGluon 내부 metric(end_x / end_y 각각 rmse vs mae 중 무엇이 더 유리한지)의 최적 조합**  \n",
    " 후보: (x=rmse, y=rmse) vs (x=rmse, y=mae)\n",
    " - baseline feature + (rmse, rmse)\n",
    " - baseline feature + (rmse, mae)\n",
    " - pruned feature + (rmse, rmse)\n",
    " - pruned feature + (rmse, mae)  \n",
    " => 각각 OOF score를 구해서 최솟값을 best로 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf02ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==== STEP3: Candidates (no-branch yet) with OOF cache ====\")\n",
    "\n",
    "candidates = []\n",
    "\n",
    "def eval_candidate(tag_name, fcols, x_metric, y_metric, k_prev_fixed):\n",
    "    \"\"\"\n",
    "    후보 1개를 평가(OOF score + OOF pred)하고 candidates에 넣기\n",
    "    - ✅ OOF 캐시 사용으로 중복 학습 제거\n",
    "\n",
    "    5-fold로 x, y 모델 -> 총 5 * 2회\n",
    "    \"\"\"\n",
    "    key = make_oof_key(k_prev_fixed, fcols, x_metric, y_metric, N_SPLITS, tag=\"cand\")\n",
    "    cached = load_oof_cache(key)\n",
    "\n",
    "    if cached is not None:\n",
    "        score, oof_pred = cached\n",
    "        print(f\"[CACHE HIT] {tag_name} ({key}) -> {score:.6f}\")\n",
    "    else:\n",
    "        score, oof_pred = ag_cv_score_xy(\n",
    "            data_merged=data,\n",
    "            feat_cols=fcols,\n",
    "            groups=groups,\n",
    "            x_metric=x_metric,\n",
    "            y_metric=y_metric,\n",
    "            n_splits=N_SPLITS,\n",
    "            presets=SEARCH_PRESETS,\n",
    "            time_limit=TIME_LIMIT_PER_FOLD,\n",
    "            num_gpus=NUM_GPUS,\n",
    "            fold_fitting_strategy=FOLD_FITTING_STRATEGY,\n",
    "        )\n",
    "        \n",
    "        save_oof_cache(key, (score, oof_pred))\n",
    "        print(f\"[TRAINED]   {tag_name} ({key}) -> {score:.6f}\")\n",
    "\n",
    "    candidates.append((tag_name, fcols, x_metric, y_metric, score, oof_pred, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0cb83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "eval_candidate(\"baseline_rmse_rmse\", feat_cols,        \"rmse\", \"rmse\", best_k_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc76ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_candidate(\"baseline_rmse_mae\",  feat_cols,        \"rmse\", \"mae\", best_k_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504ebdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruned\n",
    "eval_candidate(\"pruned_rmse_rmse\",   pruned_features,  \"rmse\", \"rmse\", best_k_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ae36be",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_candidate(\"pruned_rmse_mae\",    pruned_features,  \"rmse\", \"mae\", best_k_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c971f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = min(candidates, key=lambda x: x[4])  # score 기준\n",
    "best_name, best_fcols, best_xm, best_ym, base_score, base_oof = best\n",
    "print(\"\\nBest candidate (no-branch yet):\", best_name, \"score:\", base_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2559e9",
   "metadata": {},
   "source": [
    "#### STEP 4: result_name 분기 모델 비교 (best candidate 기준)\n",
    "**Pass 결과(Successful vs Unsuccessful)별로 모델을 분리하여 학습하는 Branching 전략이 단일 모델보다 성능이 좋은지 확인**  \n",
    "분기 모델이 더 좋으면 use_branch=True, 아니면 base 후보 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f3a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==== STEP4: Branching check ====\")\n",
    "\n",
    "use_branch = False\n",
    "branch_score = float(\"inf\")\n",
    "branch_oof = None\n",
    "\n",
    "# data에 result_name 컬럼이 있는 경우에만 branching을 시도\n",
    "if \"result_name\" in data.columns:\n",
    "    # branching은 구조가 달라 캐시 키를 별도로 둠(원하면 캐시 가능)\n",
    "    branch_score, branch_oof = ag_cv_score_xy_branch(\n",
    "        data_merged=data,\n",
    "        feat_cols=best_fcols,\n",
    "        groups=groups,\n",
    "        x_metric=best_xm,\n",
    "        y_metric=best_ym,\n",
    "        result_col=\"result_name\",\n",
    "        n_splits=N_SPLITS,\n",
    "        presets=SEARCH_PRESETS,\n",
    "        time_limit=TIME_LIMIT_PER_FOLD,\n",
    "        min_side=50,\n",
    "        num_gpus=NUM_GPUS,\n",
    "        fold_fitting_strategy=FOLD_FITTING_STRATEGY,\n",
    "    )\n",
    "\n",
    "    print(\"Base best score:\", base_score)\n",
    "    print(\"Branch score  :\", branch_score)\n",
    "\n",
    "    use_branch = branch_score < base_score\n",
    "    print(\"Use branching?\", use_branch)\n",
    "else:\n",
    "    print(\"[INFO] data has no result_name -> skip branching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275bb1b2",
   "metadata": {},
   "source": [
    "#### STEP 5: 후처리 튜닝 (OOF 기반 grid search)\n",
    "**예측 좌표를 경기장의 특성에 맞게 보정하여 성능을 미세 조정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bd77fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==== STEP5: Postprocess tuning ====\")\n",
    "\n",
    "# 분기 사용 여부에 따라 사용할 OOF를 선택\n",
    "if use_branch:\n",
    "    oof_use = branch_oof\n",
    "    base_ref = branch_score\n",
    "else:\n",
    "    oof_use = base_oof\n",
    "    base_ref = base_score\n",
    "\n",
    "# grid 탐색 범위\n",
    "grid_forward = [0.90, 0.95, 1.00, 1.05]\n",
    "grid_lateral = [0.80, 0.90, 1.00]\n",
    "\n",
    "y_true = data[[\"end_x\", \"end_y\"]].values\n",
    "\n",
    "# 초기값은 “후처리 안 함”\n",
    "best_pp = {\"forward_scale\": 1.0, \"lateral_shrink\": 1.0, \"score\": base_ref}\n",
    "\n",
    "# grid search: OOF에 후처리 적용 후 euclidean 최소 조합 찾기\n",
    "for fs in grid_forward:\n",
    "    for ls in grid_lateral:\n",
    "        adj = apply_postprocess(data, oof_use, fs, ls)\n",
    "        s = euclidean_mean_distance(y_true, adj)\n",
    "        if s < best_pp[\"score\"]:\n",
    "            best_pp = {\"forward_scale\": fs, \"lateral_shrink\": ls, \"score\": s}\n",
    "\n",
    "print(\"Best postprocess:\", best_pp)\n",
    "\n",
    "# 후처리 적용이 실제로 이득이면 사용\n",
    "use_postprocess = best_pp[\"score\"] < base_ref\n",
    "print(\"Use postprocess?\", use_postprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a2316b",
   "metadata": {},
   "source": [
    "#### STEP 6: 최종 학습(전체 데이터) + 저장 + submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df34f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = {\n",
    "    \"k_prev\": int(best_k_prev),\n",
    "    \"feature_set\": best_name,\n",
    "    \"x_metric\": best_xm,\n",
    "    \"y_metric\": best_ym,\n",
    "    \"use_branch\": bool(use_branch),\n",
    "    \"use_postprocess\": bool(use_postprocess),\n",
    "    \"postprocess\": best_pp if use_postprocess else {\"forward_scale\": 1.0, \"lateral_shrink\": 1.0},\n",
    "    \"presets_k_search\": SEARCH_PRESETS_K,\n",
    "    \"presets_search\": SEARCH_PRESETS,\n",
    "    \"presets_final\": FINAL_PRESETS,\n",
    "    \"fold_fitting_strategy\": FOLD_FITTING_STRATEGY,\n",
    "    \"num_gpus\": int(NUM_GPUS),\n",
    "}\n",
    "print(\"\\nFINAL CONFIG:\", final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c0e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) 최종 train 준비 (+ 전처리 일관 적용)\n",
    "train_full_x = fill_object_missing(data[best_fcols + [\"end_x\"]].copy(), cols=best_fcols)\n",
    "train_full_y = fill_object_missing(data[best_fcols + [\"end_y\"]].copy(), cols=best_fcols)\n",
    "\n",
    "# ✅ 최종학습도 object 결측 일관 처리\n",
    "train_full_x = fill_object_missing(train_full_x, cols=best_fcols)\n",
    "train_full_y = fill_object_missing(train_full_y, cols=best_fcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4122cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) 테스트 준비 (+ 전처리)\n",
    "Xt = fill_object_missing(X_test[best_fcols].copy(), cols=best_fcols)\n",
    "\n",
    "# ✅ 테스트에 result_name 없으면 branching 불가 -> 자동 해제\n",
    "if use_branch and (\"result_name\" not in X_test.columns):\n",
    "    print(\"[WARN] X_test has no result_name. Disable branching.\")\n",
    "    use_branch = False\n",
    "    final[\"use_branch\"] = False\n",
    "\n",
    "def fit_kwargs_final():\n",
    "    \"\"\"최종 .fit()에 넘길 kwargs를 None 없이 안전하게 구성.\"\"\"\n",
    "    kw = dict(\n",
    "        presets=FINAL_PRESETS,\n",
    "        ag_args_fit={\"num_gpus\": int(NUM_GPUS)},\n",
    "    )\n",
    "    if FOLD_FITTING_STRATEGY is not None:\n",
    "        kw[\"ag_args_ensemble\"] = {\"fold_fitting_strategy\": FOLD_FITTING_STRATEGY}\n",
    "    return kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dc8631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) 최종 학습 및 예측\n",
    "if not use_branch:\n",
    "    # 단일 모델(분기 없음): end_x predictor 1개 + end_y predictor 1개\n",
    "    px_pred = TabularPredictor(\n",
    "        label=\"end_x\", problem_type=\"regression\", eval_metric=best_xm,\n",
    "        path=os.path.join(MODEL_DIR, \"predictor_endx\"),\n",
    "    ).fit(train_data=train_full_x, **fit_kwargs_final())\n",
    "\n",
    "    py_pred = TabularPredictor(\n",
    "        label=\"end_y\", problem_type=\"regression\", eval_metric=best_ym,\n",
    "        path=os.path.join(MODEL_DIR, \"predictor_endy\"),\n",
    "    ).fit(train_data=train_full_y, **fit_kwargs_final())\n",
    "\n",
    "    px = px_pred.predict(Xt).values\n",
    "    py = py_pred.predict(Xt).values\n",
    "\n",
    "else:\n",
    "    # 분기 모델: 성공/실패 각각 end_x/end_y predictor (총 4개)\n",
    "    is_s = data[\"result_name\"].astype(str).eq(\"Successful\")\n",
    "    is_u = ~is_s\n",
    "\n",
    "    # fallback: 한쪽이 너무 작으면(안전장치) 전체 데이터로 단일 모델 학습\n",
    "    if is_s.sum() < 50 or is_u.sum() < 50:\n",
    "        print(\"[WARN] One side too small -> fallback to single model\")\n",
    "\n",
    "        px_all = TabularPredictor(\n",
    "            label=\"end_x\", problem_type=\"regression\", eval_metric=best_xm,\n",
    "            path=os.path.join(MODEL_DIR, \"predictor_endx_all\"),\n",
    "        ).fit(train_data=train_full_x, **fit_kwargs_final())\n",
    "\n",
    "        py_all = TabularPredictor(\n",
    "            label=\"end_y\", problem_type=\"regression\", eval_metric=best_ym,\n",
    "            path=os.path.join(MODEL_DIR, \"predictor_endy_all\"),\n",
    "        ).fit(train_data=train_full_y, **fit_kwargs_final())\n",
    "\n",
    "        px = px_all.predict(Xt).values\n",
    "        py = py_all.predict(Xt).values\n",
    "\n",
    "    else:\n",
    "        # 학습 데이터도 전처리 일관 유지\n",
    "        tr_s_x = fill_object_missing(data.loc[is_s, best_fcols + [\"end_x\"]].copy(), cols=best_fcols)\n",
    "        tr_s_y = fill_object_missing(data.loc[is_s, best_fcols + [\"end_y\"]].copy(), cols=best_fcols)\n",
    "        tr_u_x = fill_object_missing(data.loc[is_u, best_fcols + [\"end_x\"]].copy(), cols=best_fcols)\n",
    "        tr_u_y = fill_object_missing(data.loc[is_u, best_fcols + [\"end_y\"]].copy(), cols=best_fcols)\n",
    "\n",
    "        # 성공 모델\n",
    "        px_s = TabularPredictor(\n",
    "            label=\"end_x\", problem_type=\"regression\", eval_metric=best_xm,\n",
    "            path=os.path.join(MODEL_DIR, \"predictor_endx_success\"),\n",
    "        ).fit(train_data=tr_s_x, **fit_kwargs_final())\n",
    "\n",
    "        py_s = TabularPredictor(\n",
    "            label=\"end_y\", problem_type=\"regression\", eval_metric=best_ym,\n",
    "            path=os.path.join(MODEL_DIR, \"predictor_endy_success\"),\n",
    "        ).fit(train_data=tr_s_y, **fit_kwargs_final())\n",
    "\n",
    "        # 실패 모델\n",
    "        px_u = TabularPredictor(\n",
    "            label=\"end_x\", problem_type=\"regression\", eval_metric=best_xm,\n",
    "            path=os.path.join(MODEL_DIR, \"predictor_endx_unsuccess\"),\n",
    "        ).fit(train_data=tr_u_x, **fit_kwargs_final())\n",
    "\n",
    "        py_u = TabularPredictor(\n",
    "            label=\"end_y\", problem_type=\"regression\", eval_metric=best_ym,\n",
    "            path=os.path.join(MODEL_DIR, \"predictor_endy_unsuccess\"),\n",
    "        ).fit(train_data=tr_u_y, **fit_kwargs_final())\n",
    "\n",
    "        # 테스트 라우팅 (테스트에 result_name 있어야 여기까지 옴)\n",
    "        is_s_test = X_test[\"result_name\"].astype(str).eq(\"Successful\").values\n",
    "\n",
    "        # 예측 (전체를 예측한 뒤 라우팅 — 여기서는 최종 1회라서 단순 유지)\n",
    "        pred_s_x = px_s.predict(Xt).values\n",
    "        pred_s_y = py_s.predict(Xt).values\n",
    "        pred_u_x = px_u.predict(Xt).values\n",
    "        pred_u_y = py_u.predict(Xt).values\n",
    "\n",
    "        px = np.where(is_s_test, pred_s_x, pred_u_x)\n",
    "        py = np.where(is_s_test, pred_s_y, pred_u_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18274a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) 클리핑 + (선택) 후처리\n",
    "px, py = clip_xy(px, py)\n",
    "pred = np.column_stack([px, py])\n",
    "\n",
    "if use_postprocess:\n",
    "    pred = apply_postprocess(\n",
    "        df_feat=X_test,  # start_x 필요\n",
    "        pred_xy=pred,\n",
    "        forward_scale=best_pp[\"forward_scale\"],\n",
    "        lateral_shrink=best_pp[\"lateral_shrink\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724bbc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) submission 저장\n",
    "sub = pd.DataFrame({\n",
    "    \"game_episode\": X_test[\"game_episode\"].values,\n",
    "    \"end_x\": pred[:, 0],\n",
    "    \"end_y\": pred[:, 1],\n",
    "})\n",
    "\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSaved submission.csv\")\n",
    "print(sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (6) meta 저장\n",
    "meta_path = os.path.join(MODEL_DIR, \"model_meta.json\")\n",
    "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        **final,\n",
    "        \"oof_base_or_branch_score\": float(base_ref),\n",
    "        \"oof_after_postprocess_score\": float(best_pp[\"score\"]) if use_postprocess else float(base_ref),\n",
    "        \"feature_cols\": best_fcols,\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved meta:\", meta_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
